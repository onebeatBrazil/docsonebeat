{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"teste","text":"<p>1beat documentation</p> <p></p> <p>Teste </p>"},{"location":"Abertura_de_tasks/","title":"2. Abertura de tasks","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 8 de agosto de 2025 09:46 Categoria: 1. Informativo \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:10 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: CS Lingua: Ingl\u00eas, Portugu\u00eas Status: Conclu\u00eddo Texto: 8 de agosto de 2025 09:46 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Abertura_de_tasks/#1-portugues-br","title":"1 - Portugu\u00eas - BR","text":""},{"location":"Abertura_de_tasks/#guia-para-abertura-de-tasks-no-clickup-padrao-de-qualidade-e-clareza","title":"\ud83d\udccc Guia para Abertura de Tasks no ClickUp \u2013 Padr\u00e3o de Qualidade e Clareza","text":"<p>Este documento define como abrir uma task de forma eficiente para que o time t\u00e9cnico possa entender e executar rapidamente a solicita\u00e7\u00e3o.</p> <p>O objetivo \u00e9 eliminar retrabalho, reduzir tempo de investiga\u00e7\u00e3o e evitar trocas desnecess\u00e1rias de mensagens para coletar informa\u00e7\u00f5es b\u00e1sicas.</p>"},{"location":"Abertura_de_tasks/#1-titulo-da-task","title":"1. T\u00edtulo da Task","text":"<ul> <li>O t\u00edtulo deve ser curto, objetivo e autoexplicativo.</li> <li>Evite frases gen\u00e9ricas como \"Erro na tela nova\" ou \"Arrumar procedure\".</li> <li>Prefira algo como:<ul> <li><code>Tela de Estoque \u2013 View V_Inventory n\u00e3o atualiza ap\u00f3s execu\u00e7\u00e3o da SP_UpdateStock</code></li> <li><code>Adicionar SP_CalculateSales ao Job ap\u00f3s SP_UpdateInventory</code></li> </ul> </li> </ul> <p>Lembre-se: O t\u00edtulo deve permitir que qualquer pessoa entenda o que ser\u00e1 feito sem precisar abrir a descri\u00e7\u00e3o.</p>"},{"location":"Abertura_de_tasks/#2-descricao-da-task","title":"2. Descri\u00e7\u00e3o da Task","text":"<p>A descri\u00e7\u00e3o \u00e9 obrigat\u00f3ria.</p> <p>Tasks sem descri\u00e7\u00e3o n\u00e3o ser\u00e3o feitas.</p> <p>A descri\u00e7\u00e3o deve conter:</p> <ol> <li>Contexto detalhado do problema ou solicita\u00e7\u00e3o.</li> <li>Passos j\u00e1 realizados na investiga\u00e7\u00e3o (quando for um erro ou problema t\u00e9cnico).</li> <li>Informa\u00e7\u00f5es t\u00e9cnicas relevantes:<ul> <li>Nome da view envolvida.</li> <li>Nome da tabela que alimenta a view.</li> <li>Nome da procedure que atualiza a tabela.</li> <li>Origem dos dados (integra\u00e7\u00e3o, job, API, etc.).</li> </ul> </li> <li>Evid\u00eancias: prints, v\u00eddeos, arquivos de log, queries utilizadas, etc.</li> <li>Impacto: descrever o que acontece se n\u00e3o for resolvido e quem \u00e9 afetado.</li> </ol> <p>Exemplo de descri\u00e7\u00e3o ruim:</p> <p>A tela de pedidos n\u00e3o funciona. </p> <p>Exemplo de descri\u00e7\u00e3o boa:</p> <p>Na tela de Pedidos (OrderScreen), os registros n\u00e3o s\u00e3o exibidos.</p> <p>J\u00e1 investigado:</p> <ul> <li>View: <code>V_Orders</code> n\u00e3o retorna dados para o per\u00edodo atual.</li> <li>Tabela: <code>TB_Orders</code> est\u00e1 vazia.</li> <li> <p>Procedure: <code>SP_UpdateOrders</code> n\u00e3o executou no \u00faltimo Job_DailySync.</p> <p>Logs anexados no arquivo <code>SP_UpdateOrders_Log_2025-08-08.txt</code>.</p> <p>Impacto: usu\u00e1rios n\u00e3o conseguem visualizar pedidos gerados hoje. </p> </li> </ul>"},{"location":"Abertura_de_tasks/#3-uso-da-tag-urgente","title":"3. Uso da Tag \"URGENTE\"","text":"<ul> <li>Use APENAS para casos cr\u00edticos que exigem interven\u00e7\u00e3o imediata.</li> <li>Exemplos de urgente:<ul> <li>Sistema inteiro fora do ar.</li> <li>Processos cr\u00edticos de integra\u00e7\u00e3o parados.</li> <li>Perda de dados em produ\u00e7\u00e3o.</li> </ul> </li> <li>Casos cotidianos, melhorias ou corre\u00e7\u00f5es que n\u00e3o afetam de forma imediata e grave o neg\u00f3cio n\u00e3o devem ser marcados como urgentes.</li> <li>Se tudo for urgente, nada \u00e9 urgente.</li> </ul>"},{"location":"Abertura_de_tasks/#4-uso-de-arquivos-e-imagens","title":"4. Uso de Arquivos e Imagens","text":"<ul> <li>Sempre que poss\u00edvel, anexe prints, v\u00eddeos, arquivos de log ou exporta\u00e7\u00f5es.</li> <li>Isso acelera o entendimento e reduz d\u00favidas.</li> <li>Se for erro de procedure ou SQL, anexar tamb\u00e9m:<ul> <li>Query utilizada para valida\u00e7\u00e3o.</li> <li>Resultado da query.</li> <li>Mensagens de erro.</li> </ul> </li> </ul>"},{"location":"Abertura_de_tasks/#5-registro-de-demandas","title":"5. Registro de Demandas","text":"<ul> <li>Toda e qualquer demanda deve estar registrada como uma task no ClickUp.</li> <li>Solicita\u00e7\u00f5es informais como \"faz rapidinho pra mim\" ou \"\u00e9 s\u00f3 uma coisinha\" n\u00e3o ser\u00e3o executadas sem task aberta.</li> <li>Isso garante rastreabilidade e organiza\u00e7\u00e3o.</li> </ul>"},{"location":"Abertura_de_tasks/#6-tasks-para-adicao-de-procedures-em-jobs","title":"6. Tasks para Adi\u00e7\u00e3o de Procedures em Jobs","text":"<p>Quando solicitar inclus\u00e3o de uma procedure em um job, informar obrigatoriamente:</p> <ol> <li>O que a procedure faz (descri\u00e7\u00e3o funcional).</li> <li>Ap\u00f3s qual procedure ela deve ser executada no job.</li> <li>Tempo m\u00e9dio de execu\u00e7\u00e3o da procedure.</li> <li>Se a procedure \u00e9 da integra\u00e7\u00e3o Onebeat ou do Analytics.</li> </ol> <p>Exemplo:</p> <p>Solicita\u00e7\u00e3o: Adicionar SP_UpdatePrices ao Job_DailySync.</p> <ul> <li>Fun\u00e7\u00e3o: atualiza pre\u00e7os promocionais nas tabelas de vendas.</li> <li>Deve rodar ap\u00f3s: <code>SP_UpdateInventory</code>.</li> <li>Tempo m\u00e9dio de execu\u00e7\u00e3o: 2 minutos.</li> <li>Origem: Integra\u00e7\u00e3o Onebeat.</li> </ul> <p>\u26a0\ufe0f Importante: Sem essas informa\u00e7\u00f5es, a adi\u00e7\u00e3o ser\u00e1 ignorada.</p>"},{"location":"Abertura_de_tasks/#resumo-checklist-antes-de-abrir-a-task","title":"Resumo \u2013 Checklist Antes de Abrir a Task","text":"<p>\u2705 T\u00edtulo claro e objetivo.</p> <p>\u2705 Descri\u00e7\u00e3o t\u00e9cnica e detalhada.</p> <p>\u2705 Informa\u00e7\u00f5es sobre views, tabelas e procedures.</p> <p>\u2705 Prints, arquivos ou logs anexados.</p> <p>\u2705 Uso correto da tag URGENTE.</p> <p>\u2705 Registro obrigat\u00f3rio no ClickUp (nada informal).</p> <p>\u2705 Especifica\u00e7\u00f5es completas para altera\u00e7\u00f5es de jobs/procedures.</p> <p>Se seguirmos este padr\u00e3o, conseguiremos reduzir o tempo de resolu\u00e7\u00e3o em at\u00e9 10x e evitar gargalos de comunica\u00e7\u00e3o entre CS e Tech.</p>"},{"location":"Abertura_de_tasks/#2-english-eua","title":"2 - English - EUA","text":""},{"location":"Abertura_de_tasks/#guide-for-opening-tasks-in-clickup-quality-and-clarity-standard","title":"\ud83d\udccc Guide for Opening Tasks in ClickUp \u2013 Quality and Clarity Standard","text":"<p>This document defines how to open a task efficiently so the tech team can quickly understand and execute the request.</p> <p>The goal is to eliminate rework, reduce investigation time, and avoid unnecessary message exchanges to gather basic information.</p>"},{"location":"Abertura_de_tasks/#1-task-title","title":"1. Task Title","text":"<ul> <li>The title must be short, objective, and self-explanatory.</li> <li>Avoid generic phrases like \"Error on the new screen\" or \"Fix procedure\".</li> <li>Prefer something like:<ul> <li><code>Stock Screen \u2013 View V_Inventory does not update after executing SP_UpdateStock</code></li> <li><code>Add SP_CalculateSales to Job after SP_UpdateInventory</code></li> </ul> </li> </ul> <p>Remember: The title should allow anyone to understand what will be done without opening the description.</p>"},{"location":"Abertura_de_tasks/#2-task-description","title":"2. Task Description","text":"<p>The description is mandatory.</p> <p>Tasks without description will not be done.</p> <p>The description must contain:</p> <ol> <li>Detailed context of the problem or request.</li> <li>Steps already taken in the investigation (when it is an error or technical problem).</li> <li>Relevant technical information:<ul> <li>Name of the view involved.</li> <li>Name of the table feeding the view.</li> <li>Name of the procedure updating the table.</li> <li>Data source (integration, job, API, etc.).</li> </ul> </li> <li>Evidence: screenshots, videos, log files, queries used, etc.</li> <li>Impact: describe what happens if not solved and who is affected.</li> </ol> <p>Example of a bad description:</p> <p>The order screen is not working. </p> <p>Example of a good description:</p> <p>On the Orders screen (OrderScreen), records are not displayed.</p> <p>Already investigated:</p> <ul> <li>View: <code>V_Orders</code> does not return data for the current period.</li> <li>Table: <code>TB_Orders</code> is empty.</li> <li> <p>Procedure: <code>SP_UpdateOrders</code> did not run in the last Job_DailySync.</p> <p>Logs attached in file <code>SP_UpdateOrders_Log_2025-08-08.txt</code>.</p> <p>Impact: users cannot see orders generated today. </p> </li> </ul>"},{"location":"Abertura_de_tasks/#3-use-of-the-urgent-tag","title":"3. Use of the \"URGENT\" Tag","text":"<ul> <li>Use ONLY for critical cases requiring immediate intervention.</li> <li>Examples of urgent:<ul> <li>Entire system down.</li> <li>Critical integration processes stopped.</li> <li>Data loss in production.</li> </ul> </li> <li>Everyday cases, improvements, or fixes that do not immediately and severely affect the business should NOT be marked as urgent.</li> <li>If everything is urgent, nothing is urgent.</li> </ul>"},{"location":"Abertura_de_tasks/#4-use-of-files-and-images","title":"4. Use of Files and Images","text":"<ul> <li>Whenever possible, attach screenshots, videos, log files, or exports.</li> <li>This speeds up understanding and reduces doubts.</li> <li>If it is a procedure or SQL error, also attach:<ul> <li>Query used for validation.</li> <li>Query result.</li> <li>Error messages.</li> </ul> </li> </ul>"},{"location":"Abertura_de_tasks/#5-demand-registration","title":"5. Demand Registration","text":"<ul> <li>Every single demand must be registered as a task in ClickUp.</li> <li>Informal requests like \"do it quickly for me\" or \"it's just a little thing\" will not be executed without an opened task.</li> <li>This ensures traceability and organization.</li> </ul>"},{"location":"Abertura_de_tasks/#6-tasks-for-adding-procedures-to-jobs","title":"6. Tasks for Adding Procedures to Jobs","text":"<p>When requesting the inclusion of a procedure in a job, mandatory to inform:</p> <ol> <li>What the procedure does (functional description).</li> <li>After which procedure it should run in the job.</li> <li>Average execution time of the procedure.</li> <li>Whether the procedure is from Onebeat integration or Analytics.</li> </ol> <p>Example:</p> <p>Request: Add SP_UpdatePrices to Job_DailySync.</p> <ul> <li>Function: updates promotional prices in sales tables.</li> <li>Must run after: <code>SP_UpdateInventory</code>.</li> <li>Average execution time: 2 minutes.</li> <li>Origin: Onebeat Integration.</li> </ul> <p>\u26a0\ufe0f Important: Without this information, the addition will be ignored.</p>"},{"location":"Abertura_de_tasks/#summary-checklist-before-opening-the-task","title":"Summary \u2013 Checklist Before Opening the Task","text":"<p>\u2705 Clear and objective title.</p> <p>\u2705 Technical and detailed description.</p> <p>\u2705 Information about views, tables, and procedures.</p> <p>\u2705 Attached screenshots, files, or logs.</p> <p>\u2705 Correct use of the URGENT tag.</p> <p>\u2705 Mandatory registration in ClickUp (no informal requests).</p> <p>\u2705 Complete specifications for job/procedure changes.</p> <p>If we follow this standard, we will be able to reduce resolution time by up to 10x and avoid communication bottlenecks between CS and Tech.</p>"},{"location":"Ativacao_licenca/","title":"Instala\u00e7\u00e3o da licen\u00e7a","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 23 de abril de 2025 11:58 Categoria: Instala\u00e7\u00e3o \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:20 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: CS, DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 23 de abril de 2025 11:58 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p> <p>Antes de qualquer verifique a vers\u00e3o que se encontra no Onebeat administrator:</p> <p>Se estiver na 1.7:</p> <p></p> <p>Fa\u00e7a o processo de Renovar</p> <p>Se estiver em vers\u00f5es anteriores:</p> <p></p> <p>Fa\u00e7a o processo de Instala\u00e7\u00e3o da nova licen\u00e7a.</p>"},{"location":"Ativacao_licenca/#renovar-licenca-do-onebeat","title":"Renovar licen\u00e7a do Onebeat","text":"<ul> <li>Passo 1 - Pedir a nova licen\u00e7a</li> </ul> <p>Pe\u00e7a a licen\u00e7a nos seguintes termos para support@1beat.zendesk.com:</p> <p>Segue texto padr\u00e3o:</p> <p>Renew License -</p> <p>Hi Guys,</p> <p>We will need to renew the XXXXX license.</p> <p>Onebeat Server ID: XXXXX</p> <p>Onebeat Server Activation ID: XXXXX</p> <p>Obs: Esses dois dados se encontram no Onebeat Server Administration, na aba server ID:</p> <p></p> <p>Obs: N\u00e3o se esque\u00e7a de anexar a licen\u00e7a antiga no email:</p> <p></p> <ul> <li>Passo 2 - Desligue o servi\u00e7o do Onebeat</li> </ul> <p>V\u00e1 em services e desligue o Onebeat server, tamb\u00e9m d\u00ea Quit no atalho (canto inferior direito)</p> <p></p> <p></p> <ul> <li>Passo 3 - Substituir a licen\u00e7a</li> </ul> <p>Pegue a nova licen\u00e7a do email e adicione na pasta:</p> <p>\\Program Files\\Onebeat\\Onebeat Server</p> <p>Obs: Depois de apagar a antiga</p> <ul> <li>Passo 4 - Reiniciar o servi\u00e7o</li> </ul> <p>Reinicie o servi\u00e7o do onebeat server ligando e desligando por 10 segundos</p> <ul> <li>Passo 5 - Verifica\u00e7\u00e3o</li> </ul> <p>Abra o onebeat server manager para ver se est\u00e1 running</p> <p>Abra o Onebeat administrator e verifique se a licen\u00e7a foi aplicada corretamente</p>"},{"location":"Ativacao_licenca/#instalacao-da-nova-licenca","title":"Instala\u00e7\u00e3o da nova licen\u00e7a","text":"<ul> <li>Passo 1 - Pedir a licen\u00e7a</li> </ul> <p>Pe\u00e7a a licen\u00e7a nos seguintes termos:</p> <p></p> <p>Segue texto padr\u00e3o:</p> <p>New License - </p> <p>Hi Guys,</p> <p>We gonna need a new license for XXXXX. We'll running in one server from our side.</p> <p>We need to add:</p> <p>Unlimited databases, Unlimited Users, Also the new feature\u00a0Open Server ID</p> <p>Onebeat Server ID:\u00a0XXXXX</p> <p>Onebeat Server Activation ID:\u00a0XXXXX</p> <p>Obs: N\u00e3o se esqueca de anexar a licenca antiga no email.</p> <ul> <li>Passo 2 - Baixe o Onebeat novo</li> </ul> <p>Segue link do drive: </p> <p>https://drive.google.com/file/d/1wc6SHkyJ7LmRCK9Jlq-qjb3YdtIhB4vq/view?usp=sharing</p> <p>Extraia</p> <ul> <li>Passo 3 - Desligue o servico do Onebeat</li> </ul> <p>V\u00e1 em services e desligue o Onebeat server, tambem de Quit nos atalhos (canto inferior direito)</p> <ul> <li>Passo 4 - Exclua o Onebeat</li> </ul> <p>Digite na barra de pesquisa do windows \u201cAdd or remove programs\",  segue imagem:</p> <p></p> <p>pesquise por onebeat na barra e apague os apps de cima para baixo, veja:</p> <p></p> <p>Confira se tem 7 e apague todos.</p> <p>Reinicie o servidor.</p> <ul> <li>Passo 5 - Intalacao do Onebeat</li> </ul> <p>Instale os arquivos de cima para baixo, sempre executando como administrador:</p> <p></p> <p>Obs: N\u00e3o deixe nenhuma caixinha marcada.</p> <ul> <li>Passo 6 - Alterando o Onebeat server</li> </ul> <p>Abra o services, clique com o botao direito no Onebeat server e va em properties (ele nao pode estar rodando!!!!)</p> <p>Va na aba Log On</p> <p>Selecione This account</p> <p>Selecione Browse\u2026</p> <p></p> <p>V\u00e1 em advanced\u2026</p> <p>Clique Find Now</p> <p>)</p> <p>Escolha la em baixo o usuario onebeat_dev com dois cliques.</p> <p>)</p> <p>Depois OK</p> <p>Depois apague as duas senhas desta tela:</p> <p>)</p> <p>V\u00e1 no PassBolt e pegue a senha do usuario dev e cole nos dois campos</p> <p>Clique em apply e depois em OK</p> <p>Ligue e desligue o Onebeat server e deixe desligado por no minimo 10 segundos para que as alteracoes sejam aplicadas de fato</p> <p>Essa etapa \u00e9 muito importante e deve ser feita com atencao</p> <p>Ap\u00f3s os 10 segundos ligue o Onebeat server</p> <ul> <li>Passo 7 - Nova licenca</li> </ul> <p>Pegue a nova licenca do email e adicione na pasta:</p> <p>\\Program Files\\Onebeat\\Onebeat Server</p> <p>Obs: Depois de apagar a antiga</p> <p>Reinicie o servico do onebeat server ligando e desligando por 10 segundos</p> <ul> <li>Passo 8 - Atualizar versao</li> </ul> <p>Abra o onebeat server manager para ver se est\u00e1 running</p> <p>Abra o onebeat server administrator como administrador e va na aba DB Convert</p> <p>Selecione o CLIENTE CORRETO e a ultima vers\u00e3o, deixe a caixinha preenchida:</p> <p>)</p> <p>Se for selecionado o cliente errado gerar\u00e1 um problema enorme, faca com atencao!!!</p> <p>Clique em convert</p> <p>Clique No quando aparecer a proxima janela</p> <p>Abra o Onebeat adm e confira se est\u00e1 tudo configurado.</p>"},{"location":"Automatizar_delete_backup_dos_bancos/","title":"Automatizar o delete de backup dos bancos","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 16 de julho de 2025 17:05 Categoria: Tutorial de Procedimento \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:19 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 16 de julho de 2025 17:05 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Automatizar_delete_backup_dos_bancos/#tutorial-automatizar-o-delete-de-backup-dos-bancos-de-dados","title":"Tutorial: Automatizar o Delete de Backup dos Bancos de Dados","text":""},{"location":"Automatizar_delete_backup_dos_bancos/#problema","title":"Problema","text":"<p>Os servidores Hetzner est\u00e3o apresentando problemas de espa\u00e7o em disco, o que est\u00e1 afetando as integra\u00e7\u00f5es. Esses problemas ocorrem devido ao ac\u00famulo de backups de bancos de dados que n\u00e3o s\u00e3o removidos automaticamente.</p>"},{"location":"Automatizar_delete_backup_dos_bancos/#solucao","title":"Solu\u00e7\u00e3o","text":"<p>Implementar uma rotina automatizada para deletar backups antigos dos bancos de dados, liberando espa\u00e7o em disco e evitando interrup\u00e7\u00f5es nas integra\u00e7\u00f5es.</p>"},{"location":"Automatizar_delete_backup_dos_bancos/#implementacao","title":"Implementa\u00e7\u00e3o","text":""},{"location":"Automatizar_delete_backup_dos_bancos/#passo-1-adicionar-codigo-para-remocao-de-backups","title":"Passo 1: Adicionar c\u00f3digo para remo\u00e7\u00e3o de backups","text":"<p>Adicione o seguinte bloco de c\u00f3digo ao c\u00f3digo principal:</p> <pre><code>if CONTINUE:\n    try:\n        print('REMOVING OLD BKPS')\n        autoloader_process = subprocess.Popen(\n            PATH_EXEC + 'Exec_Clean_DB_Backup.cmd',\n            cwd = PATH_EXEC,\n            text = True,\n            stdin = subprocess.PIPE,\n            stdout = subprocess.PIPE,\n            stderr = subprocess.PIPE\n        ).communicate()\n        print(autoloader_process)\n        sql_log('Removing bkps','Sucess')\n    except Exception as e: \n        sql_log('Removing bkps',str(e))\n        CONTINUE = True\n\n</code></pre> <p>No final do notebook:</p> <p></p>"},{"location":"Automatizar_delete_backup_dos_bancos/#passo-2-criar-o-arquivo-batch-para-limpeza","title":"Passo 2: Criar o arquivo batch para limpeza","text":"<p>Crie um arquivo chamado Exec_Clean_DB_Backup.cmd com o seguinte conte\u00fado:</p> <pre><code>REM ===================================================================\nREM DELETE BACKUPS Older than 5 days from D:\\BackupsFolder\nFORFILES /S /p \"D:\\BackupsFolder\"  /d -5 /C \"CMD /C echo @FILE @FDATE\" &gt;&gt; \"D:\\BackupsFolder\\Log\\DB_BACKUP_CLEAN.log\"\nFORFILES /S /p \"D:\\BackupsFolder\"  /d -5 /c \"CMD /C DEL @FILE /Q\"\nREM ===================================================================\n\n</code></pre>"},{"location":"Automatizar_delete_backup_dos_bancos/#passo-3-verificar-a-estrutura-de-diretorios","title":"Passo 3: Verificar a estrutura de diret\u00f3rios","text":"<p>Certifique-se de que:</p> <ul> <li>O diret\u00f3rio <code>D:\\BackupsFolder</code> existe</li> <li>O diret\u00f3rio <code>D:\\BackupsFolder\\Log</code> existe para armazenar os logs</li> <li>O script tem permiss\u00f5es para acessar esses diret\u00f3rios</li> </ul>  Aten\u00e7\u00e3o: Alguns clientes podem utilizar o drive C: em vez do D:. Nestes casos, adapte o script alterando o caminho para `C:\\BackupsFolder` conforme necess\u00e1rio."},{"location":"Automatizar_delete_backup_dos_bancos/#como-funciona","title":"Como funciona","text":"<p>Este processo:</p> <ul> <li>Executa automaticamente ap\u00f3s a conclus\u00e3o das rotinas principais</li> <li>Chama o arquivo batch <code>Exec_Clean_DB_Backup.cmd</code></li> <li>O batch identifica e remove todos os arquivos de backup com mais de 5 dias</li> <li>Gera um log das opera\u00e7\u00f5es em <code>D:\\BackupsFolder\\Log\\DB_BACKUP_CLEAN.log</code></li> <li>Registra o sucesso ou falha da opera\u00e7\u00e3o no log do sistema</li> </ul>"},{"location":"Automatizar_delete_backup_dos_bancos/#monitoramento","title":"Monitoramento","text":"<p>Ap\u00f3s implementar esta solu\u00e7\u00e3o, monitore:</p> <ul> <li>Espa\u00e7o em disco dos servidores Hetzner</li> <li>Logs de execu\u00e7\u00e3o para confirmar que os backups est\u00e3o sendo removidos</li> <li>Funcionamento das integra\u00e7\u00f5es</li> </ul>"},{"location":"Automatizar_delete_backup_dos_bancos/#observacoes","title":"Observa\u00e7\u00f5es","text":"<p>O par\u00e2metro <code>/d -5</code> define que apenas arquivos com mais de 5 dias ser\u00e3o exclu\u00eddos. Ajuste este valor conforme a pol\u00edtica de reten\u00e7\u00e3o de backups da empresa.</p>  Certifique-se de que os backups importantes sejam armazenados em outro local antes de implementar esta rotina de limpeza autom\u00e1tica."},{"location":"Configurar_filezila/","title":"Acceso FTP del cliente","text":"<ul> <li>Paso 1 - Descargar FileZilla</li> </ul> <p>Haz clic en el siguiente enlace para descargar el programa que te dar\u00e1 acceso a las carpetas del servidor:</p> <p>Descargar FileZilla para macOS</p> <p>Al descargar el archivo, ten cuidado de desmarcar la casilla que instala el navegador Opera.</p> <ul> <li>Paso 2 - Configura el FileZilla.</li> </ul> <p>Abre la aplicaci\u00f3n y haz clic en \"Administrador de sitios\". Mira la imagen de abajo:</p> <p></p> <p>Esquina superior izquierda.</p> <p>Haz clic en \"Nuevo sitio\".</p> <p></p> <p>Esquina superior izquierda.</p> <p>Rellena los campos con la informaci\u00f3n del servidor:</p> <p></p> <p>Haz clic en Conectar</p> <p>A la derecha aparecer\u00e1 esta pantalla si hiciste todo correctamente:</p> <p></p> <p>Haz doble clic en la carpeta InputFiles.</p> <p></p> <p>Arrastra los archivos aqu\u00ed. Recuerda arrastrar los cuatro archivos:</p> <p>STOCKLOCATIONS</p> <p>TRANSACTIONS</p> <p>STATUS</p> <p>SKUS</p> <p>Nada m\u00e1s que eso.</p>"},{"location":"Configurar_filezila/#configurar-filezila","title":"Configurar filezila","text":"<p>Creado por: Lucca Lacerda de Souza Lommez  </p> <p>Creado en: 24 de abril de 2025, 11:11  </p> <p>\u00daltima edici\u00f3n por: Lucca Lacerda de Souza Lommez  </p> <p>\u00daltima actualizaci\u00f3n: 8 de agosto de 2025, 10:20  </p> <p>Autor: Lucca Lacerda de Souza Lommez  </p> <p>Dirigido a: CS, Cliente, DEV  </p> <p>Idioma: Espa\u00f1ol  </p> <p>Estado: \u2705 Conclu\u00eddo  </p>"},{"location":"Criar_sp_BissHistory/","title":"Criar sp_BissHistory","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 14 de julho de 2025 20:28 Categoria: Tutorial de Procedimento \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:19 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 14 de julho de 2025 20:28 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Criar_sp_BissHistory/#guia-de-criacao-da-sp_bisshistory","title":"Guia de Cria\u00e7\u00e3o da sp_BissHistory","text":""},{"location":"Criar_sp_BissHistory/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este documento descreve o processo para criar a stored procedure sp_BissHistory baseada na procedure existente do cliente a sp_Biss. A sp_BissHistory \u00e9 utilizada para reconstruir a biss por algum motivo (Um exemplo \u00e9 quando precisamos reprocessar uma carga \u2014&gt; Como Reprocessar uma Carga ).</p>"},{"location":"Criar_sp_BissHistory/#etapas-para-criacao","title":"Etapas para Cria\u00e7\u00e3o","text":""},{"location":"Criar_sp_BissHistory/#1-preparacao-inicial","title":"1. Prepara\u00e7\u00e3o Inicial","text":"<ul> <li>Abra o banco de dados do cliente espec\u00edfico</li> <li>Localize e abra a sp_biss do cliente para usar como refer\u00eancia</li> </ul> <ul> <li>Crie um novo script e cole o c\u00f3digo base fornecido abaixo</li> </ul>"},{"location":"Criar_sp_BissHistory/#2-script-base","title":"2. Script Base","text":"<pre><code>---=========================================================\n-- sp_BissHistory\nUSE XXXDBOB\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\n--=========# MODA #==========\n--MPDBOB, FILIDBOB, AS2DBOB\n\n--=========# DBMP #==========\n--MPDBOB, SNDBOB, LPDBOB, KDBOB, OBDBOB, WQDBOB\n\n--=========# VEND #==========\n--GNDBOB, FILIDBOB, GSOUDBOB, USDBOB, LIVDBOB, KOEDBOB, AS2DBOB, LPDBOB\n\nCREATE PROCEDURE [dbo].[sp_BissHistory]\n@iDate nvarchar(30),\n@fDate nvarchar(30)\nAS\nBEGIN\n\n  IF OBJECT_ID('tempdb..#Biss') IS NOT NULL\n  drop table #Biss\n\n  CREATE TABLE #Biss(\n    [Data] [date] NOT NULL,\n    [stockLocationID] [int] NOT NULL,\n    [skuID] [int] NOT NULL,\n    [Buffer] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [InventoryAtSite] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [InventoryAtTransit] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [replenishmentTime] [smallint]  NOT NULL DEFAULT (0),\n    [avoidReplenishment] [bit] NOT NULL DEFAULT (0),\n    [avoidSeasonality] [bit] NOT NULL DEFAULT (0),\n    [minimumReplenishment] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [multiplications] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [Venda] [numeric](18, 5) NOT NULL,\n    [BP_Transit_Color] [nvarchar](100) NULL,\n    [BP_Site_Color] [nvarchar](100) NULL,\n    [unitPrice] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [Throughput] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [TVC] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [InventoryAtCD] [numeric](18, 5) NOT NULL DEFAULT (0),\n    [replenishmentQuantity][numeric](18, 5) NOT NULL DEFAULT (0),\n    [inventoryNeeded][numeric](18, 5) NULL DEFAULT (0),\n    [RollingSales] [numeric](18, 5) NULL,\n    [RollingSalesMoney] [numeric](18, 5) NULL,\n    --====================##====================##====================#Moda\n    -- [GradeQuebrada] [tinyint] NULL,\n    -- [GradeQuebradaRessuprimento] [tinyint] NULL,\n    -- [QuantidadeRessuprimentoFinal] [numeric](18, 5) NULL,\n    --====================##====================##====================#\n    [noConsumptionDays] [smallint] NULL,\n    [VendaSemanaCorrente] [numeric](18, 5) NULL\n  )\n\n  INSERT INTO #BISS\n          ([Data]\n          ,[stockLocationID]\n          ,[skuID]\n          ,[Buffer]\n          ,[InventoryAtSite]\n          ,[InventoryAtTransit]\n          ,[replenishmentTime]\n          ,[avoidReplenishment]\n          ,[avoidSeasonality]\n          ,[minimumReplenishment]\n          ,[multiplications]\n          ,[Venda]\n          ,[BP_Transit_Color]\n          ,[BP_Site_Color]\n          ,[unitPrice]\n          ,[Throughput]\n          ,[TVC]\n          ,[InventoryAtCD]\n          ,[replenishmentQuantity]\n          ,[inventoryNeeded]\n          ,RollingSales\n          ,RollingSalesMoney\n          --====================##====================##====================#Moda\n          -- ,GradeQuebrada\n          -- ,GradeQuebradaRessuprimento\n          -- ,QuantidadeRessuprimentoFinal\n          --====================##====================##====================#\n          ,noConsumptionDays\n          ,VendaSemanaCorrente\n        )\n    select cast(hist.updateDate as Date) as 'Data',\n      hist.stockLocationID,\n      hist.skuID,\n      hist.bufferSize,\n      hist.inventoryAtSite,\n      hist.inventoryAtTransit,\n      sl_sku.replenishmentTime,\n      sl_sku.avoidReplenishment,\n      sl_sku.avoidSeasonality,\n      sl_sku.minimumReplenishment as 'Minimum Replenishment',\n      sl_sku.multiplications as 'Multiplications',\n      hist.consumption as 'Venda',\n      CASE \n        WHEN (cast(hist.bpTransit as numeric(10, 2)) &lt; 0) THEN 'Cyan'\n        WHEN (cast(hist.bpTransit as numeric(10, 2)) &gt;=0 AND cast(hist.bpTransit as numeric(10, 2)) &lt;= hist.greenBpLevel) THEN 'Green'\n        WHEN (cast(hist.bpTransit as numeric(10, 2)) &gt;= hist.redBpLevel AND cast(hist.bpTransit as numeric(10, 2)) &lt; 1) THEN 'Red'\n        WHEN (cast(hist.bpTransit as numeric(10, 2)) = 1) THEN 'Black'\n        ELSE 'Yellow'\n        END AS 'BP_Transit_Color',\n      CASE \n        WHEN (cast(hist.bpSite as numeric(10, 2)) &lt; 0) THEN 'Cyan'\n        WHEN (cast(hist.bpSite as numeric(10, 2)) &gt;=0 AND cast(hist.bpSite as numeric(10, 2)) &lt;= hist.greenBpLevel) THEN 'Green'\n        WHEN (cast(hist.bpSite as numeric(10, 2)) &gt;= hist.redBpLevel AND cast(hist.bpSite as numeric(10, 2)) &lt; 1) THEN 'Red'\n        WHEN (cast(hist.bpSite as numeric(10, 2)) = 1) THEN 'Black'\n        ELSE 'Yellow'\n        END AS 'BP_Site_Color',\n      sl_sku.unitPrice,\n      sl_sku.Throughput,\n      sl_sku.TVC,\n      isnull(hist_origem.InventoryAtSite, 0) as 'InventoryAtDC',\n      sl_sku.replenishmentQuantity,\n      sl_sku.inventoryNeeded,\n      0 as 'RollingSales',\n      0 as 'RollingSalesMoney',\n      -- 0 as 'GradeQuebrada',                      --=========# MODA #==========\n      -- 0 as 'GradeQuebradaRessuprimento',                 --=========# MODA #==========\n      -- sl_sku.replenishmentQuantity as 'QuantidadeRessuprimentoFinal',  --=========# MODA #==========\n      sl_sku.noConsumptionDays,\n      0 as 'VendaSemanaCorrente'\n    from [dbo].[Symphony_StockLocationSkuHistory] hist\n      left join [dbo].[Symphony_StockLocations] sl on hist.stockLocationID = sl.stockLocationID\n      left join [dbo].[Symphony_SKUs] sku on hist.skuID = sku.skuID\n      inner join [dbo].[Symphony_StockLocationSkus] sl_sku on sl.stockLocationID = sl_sku.stockLocationID and sku.skuID = sl_sku.skuID\n      inner join dbo.Symphony_DBMChangePolicies policy on policy.ID = sl_sku.bufferManagementPolicy\n      left join [dbo].[Symphony_StockLocationSkuHistory] hist_origem on hist.originStockLocation = hist_origem.stockLocationID and hist.skuID = hist_origem.skuID and hist.updateDate = hist_origem.updateDate\n    --#====================##====================# CHANGE DATE\n    where cast(hist.updateDate as Date) BETWEEN @iDate and @fDate\n    --#====================##====================##====================#\n    order by hist.updateDate desc\n\n  update #BISS\n    set RollingSales = ISNULL(\n                  (\n                  select Isnull(sum(hist.consumption), 0)\n                    from [dbo].[Symphony_StockLocationSkuHistory] hist\n                  where hist.stockLocationID = #BISS.stockLocationID\n                    and hist.skuID = #BISS.skuID\n                    and hist.updateDate &gt; DATEADD(day, - 60, #BISS.Data)\n            ), 0)\n\n  update #BISS\n    set RollingSalesMoney = ISNULL(\n                  (\n                  select Isnull(sum(hist.consumption * hist.unitPrice), 0)\n                    from [dbo].[Symphony_StockLocationSkuHistory] hist\n                  where hist.stockLocationID = #BISS.stockLocationID\n                    and hist.skuID = #BISS.skuID\n                    and hist.updateDate &gt; DATEADD(day, - 60, #BISS.Data)\n            ), 0)\n\n  update #BISS\n  set VendaSemanaCorrente = ISNULL(\n          (\n          select Isnull(sum(hist.consumption), 0)\n          from [dbo].[Symphony_StockLocationSkuHistory] hist\n          where hist.stockLocationID = #BISS.stockLocationID\n          and hist.skuID = #BISS.skuID\n          and DATEPART(WEEK, hist.updateDate) = DATEPART(WEEK,getdate())\n          and DATEPART(YEAR, hist.updateDate) = DATEPART(YEAR,getdate())\n            ), 0)\n\n  INSERT INTO biss\n          ([Data]\n          ,[stockLocationID]\n          ,[skuID]\n          ,[Buffer]\n          ,[InventoryAtSite]\n          ,[InventoryAtTransit]\n          ,[replenishmentTime]\n          ,[avoidReplenishment]\n          ,[avoidSeasonality]\n          ,[minimumReplenishment]\n          ,[multiplications]\n          ,[Venda]\n          ,[BP_Transit_Color]\n          ,[BP_Site_Color]\n          ,[unitPrice]\n          ,[Throughput]\n          ,[TVC]\n          ,[InventoryAtCD]\n          ,[replenishmentQuantity]\n          ,[inventoryNeeded]\n          ,RollingSales\n          ,RollingSalesMoney\n          --====================##====================##====================#Moda\n          -- ,GradeQuebrada\n          -- ,GradeQuebradaRessuprimento\n          -- ,QuantidadeRessuprimentoFinal\n          --====================##====================##====================#\n          ,noConsumptionDays\n          ,VendaSemanaCorrente\n  )\n    SELECT * FROM #Biss\nEND\nGO\n---=========================================================\n</code></pre>"},{"location":"Criar_sp_BissHistory/#3-adaptacao-ao-cliente","title":"3. Adapta\u00e7\u00e3o ao Cliente","text":"<p>Ap\u00f3s copiar o script base, ser\u00e1 necess\u00e1rio adapt\u00e1-lo conforme a sp_biss do cliente:</p>"},{"location":"Criar_sp_BissHistory/#passo-a-passo","title":"Passo a Passo:","text":"<ul> <li>Tabela Tempor\u00e1ria: Compare as colunas do CREATE TABLE #Biss do cliente com as do script base, e adicione todas as colunas que existem na sp_biss do cliente mas n\u00e3o est\u00e3o no script base</li> <li>Primeiro INSERT: Adicione os mesmos campos ao INSERT INTO #BISS que est\u00e3o presentes na sp_biss do cliente</li> <li>Verifica\u00e7\u00e3o do SELECT: Certifique-se que todas as colunas ap\u00f3s o \"select cast(hist.updateDate as Date) as 'Data'\" estejam na mesma ordem do INSERT INTO #BISS, adicione as informa\u00e7\u00f5es faltantes</li> <li>Updates: Compare os UPDATEs da sp_biss do cliente com os do script base e certifique-se que est\u00e3o iguais</li> <li>INSERT Final: No INSERT INTO biss no final da procedure, adicione os mesmos campos que foram adicionados no INSERT INTO #BISS</li> </ul>"},{"location":"Criar_sp_BissHistory/#4-substituicoes-necessarias","title":"4. Substitui\u00e7\u00f5es Necess\u00e1rias","text":"<ul> <li>Substitua \"XXXDBOB\" pelo nome do banco de dados do cliente</li> </ul>"},{"location":"Criar_sp_BissHistory/#exemplo-final","title":"Exemplo Final","text":"<p>Ap\u00f3s todas as modifica\u00e7\u00f5es, seu script dever\u00e1 estar adaptado \u00e0s necessidades espec\u00edficas do cliente, mantendo a estrutura b\u00e1sica mas incluindo todas as particularidades necess\u00e1rias para o funcionamento correto da sp_BissHistory. Ao salvar a procedure, se ela n\u00e3o der erro, \u00e9 um excelente ind\u00edcio de que funcionou, pois ela n\u00e3o salva caso esteja faltando algo.</p>"},{"location":"Erro_PBI/","title":"Tutorial: Como Corrigir Erros no Power BI","text":""},{"location":"Erro_PBI/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este tutorial apresenta um guia passo a passo para resolver os erros mais comuns encontrados no Power BI, com foco especial em problemas relacionados ao gateway.</p>"},{"location":"Erro_PBI/#identificacao-do-erro","title":"Identifica\u00e7\u00e3o do Erro","text":"<p>O primeiro passo para resolver qualquer problema no Power BI \u00e9 identificar corretamente o erro:</p> <ul> <li>Verifique o e-mail de notifica\u00e7\u00e3o de erro enviado pelo sistema</li> <li>Analise a mensagem de erro exibida na interface do Power BI</li> <li>Os erros mais comuns est\u00e3o relacionados \u00e0 falta de atualiza\u00e7\u00e3o do gateway</li> </ul> <p></p> <p>Exemplo de erro no PBI </p> <p>(Quer saber como ler os erros do email? \u2014&gt; Verificar os erros no email )</p>"},{"location":"Erro_PBI/#procedimento-de-correcao","title":"Procedimento de Corre\u00e7\u00e3o","text":"<p>Siga estes passos para resolver o problema:</p>"},{"location":"Erro_PBI/#1-acesse-o-servidor-do-cliente","title":"1. Acesse o servidor do cliente","text":"<ul> <li>Conecte-se ao servidor do cliente que apresentou o erro</li> </ul>"},{"location":"Erro_PBI/#2-abra-o-gateway","title":"2. Abra o Gateway","text":"<ul> <li>Localize e abra o aplicativo do Gateway de Dados On-premises</li> <li>Fa\u00e7a login utilizando as credenciais do cliente</li> <li>As credenciais necess\u00e1rias podem ser encontradas no ClickUp, na aba documenta\u00e7\u00f5es.</li> </ul> <p>(Quer saber como o click up funciona? \u2014&gt; Como funciona o Click Up )</p>"},{"location":"Erro_PBI/#3-verifique-o-status-do-gateway","title":"3. Verifique o status do Gateway","text":"<ul> <li>Mantenha o gateway aberto durante o processo de solu\u00e7\u00e3o</li> <li>Verifique se o gateway est\u00e1 conectado corretamente</li> <li>Se aparecer uma mensagem solicitando atualiza\u00e7\u00e3o, clique no link fornecido que redirecionar\u00e1 para a p\u00e1gina de download da Microsoft</li> </ul> <p>Basta clicar em Dowload</p>"},{"location":"Erro_PBI/#4-execute-os-scripts-de-configuracao","title":"4. Execute os scripts de configura\u00e7\u00e3o","text":"<ul> <li>Abra o Visual Studio Code</li> <li>Execute primeiro a c\u00e9lula de configura\u00e7\u00f5es</li> </ul> <ul> <li>Em seguida, execute a(s) c\u00e9lula(s) do PBI</li> </ul> <ul> <li>Aguarde at\u00e9 que o erro seja resolvido</li> </ul>"},{"location":"Erro_PBI/#resolucao-de-problemas-comuns","title":"Resolu\u00e7\u00e3o de Problemas Comuns","text":"<p>Se o problema persistir, verifique o erro no PBI entrando na p\u00e1gina do PBI e analisar e/ou reportar ao time CS respons\u00e1vel pelo cliente.</p> <ul> <li>Clique em workspaces e em premium [nome do cliente]:</li> </ul> <p></p> <ul> <li>Clique em [nome do cliente] Dash:</li> </ul> <p>![image.png](/docs/img/erro_pbi_7.png</p> <ul> <li>Ai estar\u00e1 o erro.</li> </ul>"},{"location":"Erro_PBI/#corrigir-erro-no-pbi","title":"Corrigir erro no PBI","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 16 de julho de 2025 11:28 Categoria: Tutorial de Procedimento \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:19 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 16 de julho de 2025 11:28 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Erros_no_email/","title":"Verificar os erros no email","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 14 de julho de 2025 20:33 Categoria: 1. Informativo \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:20 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: CS, DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 14 de julho de 2025 20:33 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Erros_no_email/#guia-para-interpretacao-do-email-de-erros-diario","title":"Guia para Interpreta\u00e7\u00e3o do Email de Erros Di\u00e1rio","text":""},{"location":"Erros_no_email/#horarios-de-envio","title":"Hor\u00e1rios de Envio","text":"<p>O email de erros \u00e9 enviado diariamente nos seguintes hor\u00e1rios:</p> <ul> <li>7:00</li> <li>9:00</li> <li>11:00</li> <li>13:00</li> <li>15:00</li> <li>17:00</li> </ul>"},{"location":"Erros_no_email/#estrutura-do-cabecalho","title":"Estrutura do Cabe\u00e7alho","text":"<p>O cabe\u00e7alho do email informa quantos erros e quantos warnings foram encontrados. Erros s\u00e3o considerados graves e devem ser resolvidos imediatamente, enquanto warnings indicam problemas menos cr\u00edticos, mas que tamb\u00e9m precisam de aten\u00e7\u00e3o.</p> <p></p>"},{"location":"Erros_no_email/#sistema-de-cores-dos-clientes","title":"Sistema de Cores dos Clientes","text":"<p>Os clientes s\u00e3o apresentados com as seguintes cores:</p> <ul> <li>Verde: Indica que tudo rodou corretamente, incluindo o Autoloader e o Dashboard. Por\u00e9m, isso n\u00e3o isenta a necessidade de verificar o status das stored procedures, m\u00e9tricas e quarentenas.</li> </ul> <p></p> <ul> <li>Amarelo: Indica que o Autoloader carregou, mas o Dashboard n\u00e3o foi atualizado. Requer aten\u00e7\u00e3o para resolver os problemas. (Um exemplo: se a sp_biss n\u00e3o roda, o dash n\u00e3o atualiza.)</li> </ul> <p></p> <ul> <li>Vermelho: Indica que nem o Autoloader conseguiu rodar, impedindo completamente a carga de dados. Deve ser resolvido imediatamente.</li> </ul> <p></p>"},{"location":"Erros_no_email/#componentes-principais","title":"Componentes Principais","text":"<p>Para cada cliente, o email apresenta os seguintes componentes iniciais:</p>"},{"location":"Erros_no_email/#1-interfaces","title":"1. Interfaces","text":"<p>Respons\u00e1veis pela gera\u00e7\u00e3o de arquivos.</p>"},{"location":"Erros_no_email/#2-autoloader","title":"2. Autoloader","text":"<p>Respons\u00e1vel pelo carregamento dos arquivos no banco de dados.</p>"},{"location":"Erros_no_email/#3-jobs-db","title":"3. Jobs DB","text":"<p>Respons\u00e1veis pela execu\u00e7\u00e3o das procedures.</p>"},{"location":"Erros_no_email/#classificacao-de-erros","title":"Classifica\u00e7\u00e3o de Erros","text":"<p>A gravidade do erro depende de onde ele ocorreu:</p> <ul> <li>Erro (Vermelho): Se ocorreu antes ou durante o Autoloader. Deve ser resolvido imediatamente, pois a carga n\u00e3o foi completada.</li> <li>Warning (Amarelo): Se ocorreu do Jobs DB em diante. A carga foi feita, mas algo deu errado e deve ser corrigido.</li> </ul> <p>Para saber como corrigir todos esses erros:</p> <p>Corrigir erro na carga diaria</p>"},{"location":"Erros_no_email/#personalizacoes","title":"Personaliza\u00e7\u00f5es","text":"<p>Ap\u00f3s os componentes principais, podem existir personaliza\u00e7\u00f5es que variam de cliente para cliente:</p> <ul> <li>Reposi\u00e7\u00f5es</li> <li>Outros Autoloaders</li> <li>Outros Jobs DB</li> </ul> <p>\u00c9 importante notar que apenas o primeiro Autoloader configura erro se falhar. Os demais s\u00e3o considerados recalcula\u00e7\u00f5es e geram apenas warnings.</p>"},{"location":"Erros_no_email/#dashboards","title":"Dashboards","text":"<p>Ap\u00f3s as personaliza\u00e7\u00f5es, o email apresenta informa\u00e7\u00f5es sobre os dashboards:</p> <ul> <li>Clientes antigos: Utilizam PBI e Analytics</li> <li>Clientes novos: Utilizam apenas Analytics</li> </ul> <p>Os dashboards s\u00e3o executados por \u00faltimo. Caso ocorra algum erro, \u00e9 necess\u00e1rio analisar e resolver para garantir que as visualiza\u00e7\u00f5es estejam atualizadas com os dados mais recentes.</p> <p>Links uteis:</p> <ul> <li>Corrigir erro no PBI</li> <li>Corrigir erro no Analytics</li> </ul>"},{"location":"Erros_no_email/#procedimento-para-resolucao","title":"Procedimento para Resolu\u00e7\u00e3o","text":"<ol> <li>Priorize a resolu\u00e7\u00e3o dos erros (vermelho) antes dos warnings (amarelo)</li> <li>Concentre-se primeiro nos problemas de Autoloader, pois estes impedem a carga dos dados</li> <li>Em seguida, resolva os problemas de Jobs DB</li> <li>Por \u00faltimo, verifique os dashboards para garantir que estejam atualizados</li> </ol>"},{"location":"Erros_no_email/#informacoes-detalhadas-do-email","title":"Informa\u00e7\u00f5es Detalhadas do Email","text":"<p>Al\u00e9m das informa\u00e7\u00f5es b\u00e1sicas sobre erros e warnings, o email tamb\u00e9m apresenta estat\u00edsticas detalhadas sobre o processamento dos dados:</p>"},{"location":"Erros_no_email/#1-metricas-de-carregamento","title":"1. M\u00e9tricas de Carregamento","text":"<ul> <li>SucessLoads: N\u00famero de carregamentos bem-sucedidos</li> <li>FailLoads: N\u00famero de carregamentos que falharam</li> <li>Rows: Total de linhas carregadas no banco de dados</li> </ul>"},{"location":"Erros_no_email/#2-informacoes-de-quarentena","title":"2. Informa\u00e7\u00f5es de Quarentena","text":"<p>O email exibe o n\u00famero de registros em quarentena e os motivos. Registros em quarentena s\u00e3o aqueles que n\u00e3o puderam ser processados devido a problemas espec\u00edficos, como:</p> <ul> <li>Registros com SKUs inexistentes em locais de estoque</li> <li>Linhas duplicadas em arquivos</li> <li>Outros erros de valida\u00e7\u00e3o</li> </ul>"},{"location":"Erros_no_email/#3-arquivos-importados","title":"3. Arquivos Importados","text":"<p>Uma lista de todos os arquivos processados \u00e9 apresentada, incluindo:</p> <ul> <li>Nome do arquivo</li> <li>Data da importa\u00e7\u00e3o</li> <li>N\u00famero de registros por arquivo</li> </ul>"},{"location":"Erros_no_email/#4-tempo-de-processamento","title":"4. Tempo de Processamento","text":"<p>Para cada componente do sistema, o email mostra o hor\u00e1rio de conclus\u00e3o, permitindo identificar:</p> <ul> <li>Dura\u00e7\u00e3o total do processamento</li> <li>Componentes que est\u00e3o levando mais tempo para executar</li> </ul>"},{"location":"Erros_no_email/#5-como-interpretar-as-metricas","title":"5. Como Interpretar as M\u00e9tricas","text":"<p>Ao analisar o email, preste aten\u00e7\u00e3o especial para:</p> <ul> <li>Alto n\u00famero de quarentenas</li> <li>Discrep\u00e2ncias significativas no n\u00famero de linhas: Comparado com dias anteriores</li> <li>Tempo de processamento anormal: Componentes demorando mais que o habitual</li> </ul> <p>Essas informa\u00e7\u00f5es s\u00e3o fundamentais para diagnosticar problemas antes que afetem os dashboards e relat\u00f3rios finais.</p>"},{"location":"Erros_no_email/#verificacao-de-erros-no-log-de-integracao","title":"Verifica\u00e7\u00e3o de Erros no Log de Integra\u00e7\u00e3o","text":"<p>Para uma an\u00e1lise mais profunda e precisa dos erros relatados nos emails, \u00e9 fundamental consultar a tabela Log_Integration mencionada na 1. Overview Onebeat. Esta tabela cont\u00e9m registros detalhados de todas as integra\u00e7\u00f5es e pode fornecer informa\u00e7\u00f5es valiosas que n\u00e3o est\u00e3o dispon\u00edveis no email.</p> <p>Consultar esta tabela permite:</p> <ul> <li>Verificar se o erro ainda existe ou se j\u00e1 foi resolvido por outro membro da equipe</li> <li>Identificar se o cliente est\u00e1 gerando um \"falso positivo\" - situa\u00e7\u00f5es onde o sistema reporta um erro, mas na realidade o processo foi conclu\u00eddo com sucesso</li> <li>Obter detalhes t\u00e9cnicos mais espec\u00edficos sobre a natureza do erro</li> <li>Visualizar o hist\u00f3rico de ocorr\u00eancias semelhantes para identificar padr\u00f5es</li> </ul> <pre><code>-- Consulta b\u00e1sica para verificar os registros de integra\u00e7\u00e3o recentes\nSELECT \n   *\nFROM \n    Log_Integration\nORDER BY \n    date DESC\n</code></pre> <p>Ao analisar os resultados desta consulta, preste aten\u00e7\u00e3o especial aos campos Status e ErrorMessage, que fornecem informa\u00e7\u00f5es diretas sobre a natureza do problema e podem ajudar a determinar a abordagem correta para resolu\u00e7\u00e3o.</p> <p></p> <p>Exemplo de erro de procedure.</p>"},{"location":"Integrar_Shared_Project/","title":"Integrar Shared Project","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 6 de agosto de 2025 11:26 Categoria: Integra\u00e7\u00e3o \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 18 de agosto de 2025 13:39 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 6 de agosto de 2025 11:26 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Integrar_Shared_Project/#indice","title":"\u00cdndice","text":"<ol> <li>Introdu\u00e7\u00e3o</li> <li>Pr\u00e9-requisitos</li> <li>Vis\u00e3o Geral da Estrutura Antiga</li> <li>Objetivo da Migra\u00e7\u00e3o</li> <li>Passo a Passo da Migra\u00e7\u00e3o<ul> <li>5.1 Clonar o Shared_Project</li> <li>5.2 Criar/ajustar estrutura de pastas</li> <li>5.3 Atualizar cabe\u00e7alho do notebook</li> <li>5.4 Importar e configurar o <code>Shared_Project</code></li> <li>5.5 Converter c\u00e9lulas: Interfaces, Autoloader, SPs, Dash, Analytics, etc</li> </ul> </li> <li>Valida\u00e7\u00f5es e Testes</li> <li>Boas Pr\u00e1ticas e Checklist Final</li> </ol>"},{"location":"Integrar_Shared_Project/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<p>Este tutorial tem como objetivo guiar a migra\u00e7\u00e3o de notebooks antigos de integra\u00e7\u00e3o para o novo modelo baseado no projeto <code>Shared_Project</code>, promovendo padroniza\u00e7\u00e3o, reuso de c\u00f3digo e redu\u00e7\u00e3o de erros operacionais.</p>"},{"location":"Integrar_Shared_Project/#2-pre-requisitos","title":"2. Pr\u00e9-requisitos","text":"<ul> <li>Python 3.12 (recomendado)</li> <li>Git instalado e configurado</li> <li>Acesso ao reposit\u00f3rio <code>Shared_Project</code> no GitHub</li> <li>Estrutura existente no caminho: <code>C:/OnebeatFiles/Integrations_Git/CLIENTE/</code></li> <li><code>.env</code> atualizado com credenciais do cliente (PBI, banco local e central)</li> <li>Biblioteca <code>decouple</code> instalada</li> <li>Pasta <code>/Logs/</code> criada no projeto</li> </ul>"},{"location":"Integrar_Shared_Project/#3-visao-geral-da-estrutura-antiga","title":"3. Vis\u00e3o Geral da Estrutura Antiga","text":"<p>O notebook antigo fazia:</p> <ul> <li>Execu\u00e7\u00e3o de <code>.cmd</code></li> <li>Valida\u00e7\u00e3o manual de arquivos</li> <li>Logs manuais no SQL</li> <li>Chamadas diretas de procedures</li> <li>Requisi\u00e7\u00f5es PowerBI manuais</li> <li>Sem modulariza\u00e7\u00e3o nem tratamento padr\u00e3o de erros</li> </ul>"},{"location":"Integrar_Shared_Project/#4-objetivo-da-migracao","title":"4. Objetivo da Migra\u00e7\u00e3o","text":"<ul> <li>Reduzir duplicidade de c\u00f3digo</li> <li>Centralizar l\u00f3gica de SPs, logs, chamadas PowerBI, etc</li> <li>Usar fun\u00e7\u00f5es reutiliz\u00e1veis do <code>Shared_Project</code></li> <li>Melhorar logging e rastreabilidade</li> <li>Melhorar organiza\u00e7\u00e3o e manuten\u00e7\u00e3o dos notebooks</li> </ul>"},{"location":"Integrar_Shared_Project/#5-passo-a-passo-da-migracao","title":"5.  Passo a Passo da Migra\u00e7\u00e3o","text":""},{"location":"Integrar_Shared_Project/#51-clonar-o-shared_project","title":"5.1. Clonar o <code>Shared_Project</code>","text":"<p>Abra o terminal (VSCode ou CMD):</p> <pre><code>cd C:/OnebeatFiles/Integrations_Git/CLIENTE/\ngit clone git@github.com:onebeatBrazil/Shared_Project.git\n</code></pre>"},{"location":"Integrar_Shared_Project/#52-atualizar-cabecalho-do-notebook","title":"5.2. Atualizar cabe\u00e7alho do notebook","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport threading\nimport requests\nimport sys\nimport os\nimport gc\nimport time\nimport sqlalchemy\nimport urllib\nimport pyodbc\nimport io\nimport subprocess\nimport zipfile\nimport re\nimport decouple \nimport importlib\n#====================##====================##====================#\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_colwidth', 200)\nDECOUPLE = decouple.Config(decouple.RepositoryEnv('./.env'))\n#====================##====================##====================#\nPROCESS_DATE = dt.datetime.strftime(dt.datetime.today() - dt.timedelta(days=1),'20%y-%m-%d')\nCURRENT_DATE = dt.datetime.strftime(dt.datetime.today() - dt.timedelta(days=0),'20%y-%m-%d')\n#====================##====================##====================#\nPATH_ONEBEAT = 'C:/OnebeatFiles/'\nPATH_INPUT = f'{PATH_ONEBEAT}/InputFolder/' if os.path.isdir(f'{PATH_ONEBEAT}/InputFolder/') else './'\nPATH_HISTORY = f'{PATH_ONEBEAT}/HistoryFolder/' if os.path.isdir(f'{PATH_ONEBEAT}/HistoryFolder/') else './'\n#====================##====================##====================#\nCONNECTION_STRING = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={DECOUPLE('SERVER')};DATABASE={DECOUPLE('DATABASE')};UID={DECOUPLE('USER')};PWD={DECOUPLE('PASSWORD')}\"\nENGINE_ONEBEAT = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % urllib.parse.quote_plus(CONNECTION_STRING))\n#====================##====================##====================#\nDEBUG = False # DB Insert - File Generation - API Post\nCONTINUE = True\n#====================##====================##====================#\nfrom Shared_Project import sharedFunctions\nsharedFunctions.GitPull(); importlib.reload(sharedFunctions)\nsharedFunctions.ImportLocalVars(CONNECTION_STRING,ENGINE_ONEBEAT,CURRENT_DATE,PATH_ONEBEAT,PATH_INPUT,DECOUPLE,DEBUG)\nsharedFunctions.ServerValidation()\n</code></pre> <p>Abaixo do bloco padr\u00e3o devem ser adicionadas as personaliza\u00e7\u00f5es e, obrigatoriamente, a vari\u00e1vel OBRIGATORY_FILES (com suas devidas personaliza\u00e7\u00f5es). Veja alguns exemplos de personaliza\u00e7\u00f5es:</p> <pre><code>OBRIGATORY_FILES = {f'MTSSKUS_{PROCESS_DATE}', f'STATUS_{PROCESS_DATE}', f'TRANSACTIONS_{PROCESS_DATE}', f'STOCKLOCATIONS_{PROCESS_DATE}'}\n#====================##====================##====================# \nPATH_LOGS = f'{PATH_ONEBEAT}Integrations_Git/CLIENTE/Logs'\n##====================##====================##====================# \n#PATH_SEASONALITY = os.path.join(PATH_INPUT, f\"SEASONALITY{f\"_{CURRENT_DATE}_{dt.datetime.now().strftime('%H-%M')}\"}.csv\")\n#FTP_DATE = dt.datetime.strftime(dt.datetime.today() - dt.timedelta(days=1),'20%y%m%d')\n#FTP_OUTPUT = f'C:/OnebeatFiles/ftp-totto-guatemala/OutputFiles'\n#FTP_LIDOS = f'C:/OnebeatFiles/ftp-totto-guatemala/OutputFiles/Lidos'\n#FTP_INPUT = f'C:/OnebeatFiles/ftp-totto-guatemala/InputFiles/'\n#FTP_HISTORY = f'C:/OnebeatFiles/ftp-totto-guatemala/HistoryFiles' if os.path.isdir('C:/OnebeatFiles/inputFolder/') else f'./{FTP_DATE}'\n##====================##====================##====================#\n#sharedFunctions.resetWindowsServices(['PBIDesktop.exe','OnebeatServer','SQLSERVERAGENT','MSSQLSERVER']) # Restarting sql server services in hetzner servers\n</code></pre>"},{"location":"Integrar_Shared_Project/#53-importar-e-configurar-o-shared_project","title":"5.3. \u2705 Importar e configurar o Shared_Project","text":"<ul> <li>O notebook deve importar <code>sharedFunctions</code> e chamar o <code>GitPull()</code> + <code>ImportLocalVars()</code> antes de qualquer coisa.</li> <li>Todas as vari\u00e1veis globais usadas no projeto devem ser definidas no notebook principal.</li> </ul>"},{"location":"Integrar_Shared_Project/#54-converter-celulas-passo-a-passo","title":"5.4. \ud83d\udd04 Converter c\u00e9lulas passo a passo","text":""},{"location":"Integrar_Shared_Project/#interfaces","title":"\ud83e\udde9 Interfaces","text":"<p>Antes:</p> <pre><code># try:\n#     print('Running Interfaces')\n#     Interfaces = subprocess.Popen(\n#         PATH_EXEC + 'Exec_Interfaces.cmd',\n#         cwd = PATH_EXEC,\n#         text = True,stdin = subprocess.PIPE,stdout = subprocess.PIPE,stderr = subprocess.PIPE).communicate()\n#     print(Interfaces)\n\n#     #---------------Validation------------\n#     InputFolderFiles = [file for file in os.listdir(PATH_INPUT) if file.endswith('.csv')]\n#     HistoryFolderFiles = [file for file in os.listdir(PATH_HISTORY) if file.endswith('.csv')]\n\n#     if len(InputFolderFiles) == 0: sql_log('Interfaces','No interface generated'); CONTINUE = False\n#     elif len(InputFolderFiles) % NUM_INTERFACE_FILES != 0: sql_log('Interfaces','InputFolder incorrect number of files'); CONTINUE = False #If there's X Files in inputFolder\n#     elif any(x in InputFolderFiles for x in HistoryFolderFiles): sql_log('Interfaces','Input and History same file name'); CONTINUE = False #If no filename is in HistoryFolder\n#     if CONTINUE: sql_log('Interfaces','Sucess')\n# except Exception as e: sql_log('Interfaces',str(e)); CONTINUE = False\n</code></pre> <p>Depois:</p> <pre><code>try:\n    if CONTINUE:\n        sharedFunctions.clear_inputFolder()\n        sharedFunctions.run_executable('cliente_Interfaces.ipynb')\n\n    #====================##====================# VALIDATION\n        CONTINUE = False\n        InputFolderFiles = [file for file in os.listdir(PATH_INPUT) if file.endswith('.csv')]\n        HistoryFolderFiles = [file for file in os.listdir(PATH_HISTORY) if file.endswith('.csv')]\n\n        if OBRIGATORY_FILES - {f.split('.')[0] for f in os.listdir(PATH_INPUT)}: raise Exception('Missing Obrigatory Files')\n        elif any(x in InputFolderFiles for x in HistoryFolderFiles): raise Exception('Input and History same file name')\n        else: sharedFunctions.sql_log('Interfaces','Success'); CONTINUE = True\n    #====================#\nexcept Exception as e:sharedFunctions.sql_log('Interfaces',str(e)); CONTINUE = False\n</code></pre> <p>Nota: Substitua \"cliente\" pelo nome do cliente espec\u00edfico</p>"},{"location":"Integrar_Shared_Project/#autoloader","title":"\ud83e\udde9 Autoloader","text":"<p>Antes:</p> <pre><code># try:\n#     if CONTINUE:\n#         print('Importing Files to Onebeat')\n#         autoloader_process = subprocess.Popen(\n#             PATH_AUTOLOADER + 'AutoLoader.cmd',\n#             cwd = PATH_AUTOLOADER,\n#             text = True,stdin = subprocess.PIPE,stdout = subprocess.PIPE,stderr = subprocess.PIPE).communicate()\n#         print(autoloader_process)\n\n#         #---------------Validation------------\n#         time.sleep(60)\n#         df = pd.read_sql_query(f\"\"\"select top 1 isLoadSuccessful from Symphony_LoadRecalculateLog where startDate &gt;= '{CURRENT_DATE}' order by startDate desc\"\"\",ENGINE_ONEBEAT)\n#         if df.shape[0] &lt;= 0: sql_log('Autoloader','Missing Load &amp; Recalculate'); CONTINUE = False\n#         elif df.isLoadSuccessful[0] == False: sql_log('Autoloader','Error in Load &amp; Recalculate'); CONTINUE = False\n#         if CONTINUE: sql_log('Autoloader','Sucess')\n# except Exception as e: sql_log('Autoloader',str(e)); CONTINUE = False\n</code></pre> <p>Depois:</p> <pre><code>try:\n    if CONTINUE:\n        sharedFunctions.run_executable('AutoLoader.cmd',PATH_ONEBEAT)\n\n    #====================##====================# VALIDATION\n        CONTINUE = False\n        time.sleep(60) #Error in read DB right after L&amp;R Solution\n\n        df = pd.read_sql_query(f\"\"\"select top 1 isLoadSuccessful from Symphony_LoadRecalculateLog where startDate &gt;= '{CURRENT_DATE}' order by startDate desc\"\"\",ENGINE_ONEBEAT)\n\n        if df.empty: raise Exception('Missing Load &amp; Recalculate')\n        elif df.isLoadSuccessful[0] == False: raise Exception('Error in Load &amp; Recalculate')\n        else: sharedFunctions.sql_log('Autoloader','Success'); CONTINUE = True\n    #====================#\nexcept Exception as e:sharedFunctions.sql_log('Autoloader',str(e)); CONTINUE = False\n</code></pre>"},{"location":"Integrar_Shared_Project/#execucao-de-procedures","title":"\ud83e\udde9 Execu\u00e7\u00e3o de Procedures","text":"<p>Antes:</p> <pre><code># if CONTINUE:\n#     print('Start DBRoutines: '+ dt.datetime.strftime(dt.datetime.today(),'20%y-%m-%d %H:%M:%S'))\n#     dailyJobs = [\n#         'sp_BISS',\n#     ]\n#     for job in dailyJobs:\n#         try:\n#             print(f'Executing {job} {pd.Timestamp.today().time()}')\n#             sql_connection = pyodbc.connect(CONNECTION_STRING,autocommit=True).cursor()\n#             sql_connection.execute(job)\n#         except Exception as e:\n#             if (job == 'sp_biss') and ('Duplicate key was ignored' in str(e)): continue #Ignore sp_biss duplicate key return\n#             if (job == 'sp_AtualizaVendas') and ('Null value is eliminated by an aggregate or other SET operation' in str(e)): continue\n#             sql_log(f'DB_Routines.{job}',str(e))\n#         finally: sql_connection.close()\n\n#     sql_log('DB_Routines','Sucess')\n</code></pre> <p>Depois:</p> <pre><code>try:\n    if CONTINUE:\n\n        dailyJobs = [\n        'sp_biss',\n        ]\n\n        sharedFunctions.exec_dbJobs(dailyJobs)\n        sharedFunctions.sql_log('Jobs DB','Success')\nexcept Exception as e:sharedFunctions.sql_log('Jobs DB',str(e)); CONTINUE = False\n</code></pre> <p>Nota: adicione as procedures espec\u00edficas do cliente, o exemplo acima \u00e9 apenas ilustrativo</p>"},{"location":"Integrar_Shared_Project/#repositions","title":"\ud83e\udde9\u00a0Repositions","text":"<pre><code>if CONTINUE:\n    try:\n        sharedFunctions.run_executable('CLIENTE_Repositions.ipynb')\n    except Exception as e:sharedFunctions.sql_log('Repositions',str(e))\n</code></pre>"},{"location":"Integrar_Shared_Project/#powerbi-dashboards","title":"\ud83e\udde9 PowerBI Dashboards","text":"<p>Antes:</p> <pre><code># from msal import ConfidentialClientApplication\n# if CONTINUE:\n#     try:\n#         dash_workspace = 'XXXXXXXXXX'\n#         dash_dataset = 'XXXXXXXXXX'\n\n#         SECRET = {\n#             'client_id' : \"XXXXXXXXXX\",\n#             'client_secret' : \"XXXXXXXXXX\",\n#             'tenant_name' : \"XXXXXXXXXX\",\n#             'authority_url' : \"XXXXXXXXXX\",\n#             'scope' : [\"XXXXXXXXXX\"],\n#             'workspace_id' : dash_workspace,\n#             'dataset_id' : dash_dataset,\n#             'url' : f\"XXXXXXXXXX\"\n#         }\n\n#         app = ConfidentialClientApplication(SECRET['client_id'], authority=SECRET['authority_url'], client_credential=SECRET['client_secret'])\n#         token = app.acquire_token_for_client(scopes=SECRET['scope'])\n#         HEADER = {'Content-Type':'application/json', 'Authorization':f'Bearer {token[\"access_token\"]}'}\n#         api_call = requests.post(url=SECRET['url'], headers=HEADER) #Request PBI Refresh\n\n#         #---------------Validation------------\n#         if pd.read_sql_query(f\"\"\"if (select max(data) as data from biss) = '{PROCESS_DATE}' select 1 else select 0\"\"\",ENGINE_ONEBEAT).iloc[0,0] == 0: sql_log('PBI_Dash','Biss table not updated!'); raise\n#         if 'access_token' not in token: sql_log('PBI_Dash','Could not generate TokenAcess'); raise \n#         if api_call.status_code != 202: sql_log('PBI_Dash',f'Couldnt refresh .post dash: {api_call.status_code,api_call.reason,api_call.content}'); raise \n#         #---------------\n\n#         try:\n#             while True:\n#                 request = requests.get(url=SECRET['url'], headers=HEADER)\n\n#                 #---------------Validation------------\n#                 if request.status_code != 200: \n#                     if request.status_code == 403: \n#                         app = ConfidentialClientApplication(SECRET['client_id'], authority=SECRET['authority_url'],client_credential=SECRET['client_secret'])\n#                         token = app.acquire_token_for_client(scopes=SECRET['scope'])\n#                         HEADER = {'Content-Type':'application/json', 'Authorization':f'Bearer {token[\"access_token\"]}'}\n#                         continue\n#                     else:\n#                         sql_log('PBI_Dash',f'Error: {request.status_code,request.reason,request.content}'); raise Exception (f'{request.status_code,request.reason,request.content}')\n#                 #---------------\n\n#                 dashLog = pd.DataFrame(request.json()['value'], columns=['requestId', 'id', 'refreshType', 'startTime', 'endTime', 'status']).sort_values('startTime',ascending=False)\n\n#                 if pd.isnull(dashLog.endTime[0]) == False:\n#                         if dashLog.status[0] == \"Completed\": sql_log('PBI_Dash','Sucess'); raise Exception('Sucess')\n#                         elif dashLog.status[0] == \"Failed\": sql_log('PBI_Dash','Failed'); raise\n#                         elif dashLog.status[0] == \"Disabled\": sql_log('PBI_Dash','Dataset refresh is disabled'); raise\n#                         elif dashLog.status[0] == \"Unknown\": sql_log('PBI_Dash','Dataset is already refreshing'); raise\n#                         else: sql_log('PBI_Dash',f'Unfamiliar dashLog status {dashLog.status[0]}'); raise\n#                 else:\n#                     print('Waiting 2 minutes to check again')\n#                     time.sleep(120) #Sleep for 1minute and check again\n#         except Exception as e: raise(e)\n#     except Exception as ee: print(ee)\n</code></pre> <p>Depois:</p> <pre><code>if CONTINUE:\n    try:  \n        t1 = threading.Thread(target=sharedFunctions.pbi_refresh,args=('PBI_Dash','XXXXXXXXXX','XXXXXXXXXX'))\n        t1.start()\n    except Exception as e: sharedFunctions.sql_log('PBI_Dash',str(e))\n</code></pre> <p>Nota: substitua os valores XXXXXXXXXX pelos identificadores espec\u00edficos do dash_workspace e dash_dataset. Tamb\u00e9m substitua \"PBI_Dash\" pelo nome que deseja exibir no registro de log.</p>"},{"location":"Integrar_Shared_Project/#analytics","title":"\ud83e\udde9 Analytics","text":"<pre><code>try:\n    if CONTINUE:\n        t2 = threading.Thread(target=sharedFunctions.run_executable,args=('Analytics_Upload_data.ipynb','./Shared_Project/',))\n        t2.start()\nexcept Exception as e: sharedFunctions.sql_log('Analytics',str(e))\n</code></pre> <p>Nota: Os exemplos est\u00e3o usando t1 e t2, mas se o cliente tiver apenas o analytics, use somente t1. Se o cliente tiver mais de um dashboard, use t3, t4 ou quantos forem necess\u00e1rios. Sempre inclua o Wait threads para cada thread ap\u00f3s todos os dashboards. C\u00f3digo exemplo abaixo:</p>"},{"location":"Integrar_Shared_Project/#wait-threads","title":"\ud83e\udde9\u00a0Wait threads","text":"<pre><code>if CONTINUE:\n    t1.join()\n    t2.join()\n</code></pre>"},{"location":"Integrar_Shared_Project/#zip-backup","title":"\ud83e\udde9 Zip &amp; Backup","text":"<pre><code>try:\n    if CONTINUE:\n        sharedFunctions.zipDeleteFiles(PATH_HISTORY, daysAgo=60, fileExtension='.csv', mode='zip')\n        sharedFunctions.zipDeleteFiles(PATH_LOGS, daysAgo=60, fileExtension='.txt', mode='zip')\n        sharedFunctions.zipDeleteFiles(PATH_LOGS, daysAgo=60, fileExtension='.html', mode='zip')\n\n        if any(f.endswith('.zip') for path in [PATH_HISTORY,PATH_LOGS] for f in os.listdir(path)):\n            sharedFunctions.moveToDrive([PATH_HISTORY,PATH_LOGS], extension=\".zip\", mode=\"BR\", nomeDoCliente=\"XXXXXX\")\n            sharedFunctions.zipDeleteFiles(PATH_HISTORY, daysAgo=0, fileExtension='.zip', mode='delete')\n            sharedFunctions.zipDeleteFiles(PATH_LOGS, daysAgo=0, fileExtension='.zip', mode='delete')\nexcept Exception as e:sharedFunctions.sql_log('Zip',str(e))\n</code></pre> <p>Nota: Adicione a FTP apenas se utilizado pelo cliente, sempre ajustando a extens\u00e3o do arquivo. Backup ainda em testes.</p>"},{"location":"Integrar_Shared_Project/#shutdown","title":"\ud83e\udde9 Shutdown","text":"<pre><code>if CONTINUE:\n    try:\n        sharedFunctions.shutDownServer(lastProccess='ANALYTICS.INTEGRATION')\n    except Exception as e: sharedFunctions.sql_log('Shutdown',str(e))\n</code></pre>"},{"location":"Integrar_Shared_Project/#6-validacoes-e-testes","title":"6. \u2705 Valida\u00e7\u00f5es e Testes","text":"<ol> <li>\u2699\ufe0f Rode a c\u00e9lula de cabe\u00e7alho</li> <li>\ud83e\uddea Rode <code>Interfaces</code> e delete a transactions</li> <li>\ud83e\uddea Rode <code>Autoloader</code>, verifique <code>Symphony_LoadRecalculateLog</code></li> <li>\ud83d\udd01 Rode SPs e veja se <code>sql_log</code> grava corretamente</li> <li>\ud83d\udcca Teste dashboards PowerBI com <code>thread.join()</code></li> <li>\ud83d\udcc2 Verifique se logs <code>.html</code> est\u00e3o na pasta <code>/Logs/</code></li> <li>\ud83d\udce4 Verifique envio autom\u00e1tico ao Drive, se configurado</li> </ol>"},{"location":"Integrar_Shared_Project/#7-boas-praticas-e-checklist-final","title":"7. \ud83d\udccc Boas Pr\u00e1ticas e Checklist Final","text":"<p>\u2705 Vari\u00e1veis <code>CURRENT_DATE</code>, <code>PATH_INPUT</code>, <code>ENGINE_ONEBEAT</code> definidas</p> <p>\u2705 <code>GitPull()</code> antes de importar <code>sharedFunctions</code></p> <p>\u2705 <code>.env</code> com vari\u00e1veis do cliente</p> <p>\u2705 Scripts separados como <code>.ipynb</code>, sem necessidade de <code>.cmd</code></p> <p>\u2705 SPs padronizadas no <code>exec_dbJobs()</code></p> <p>\u2705 <code>sql_log()</code> substitu\u00eddo por <code>sharedFunctions.sql_log()</code></p> <p>\u2705 <code>Sucess</code> substitu\u00eddo por <code>Success</code></p> <p>\u2705 Manter apenas o arquivo de execu\u00e7\u00e3o main.cmd na pasta Exec</p>"},{"location":"Overview_Onebeat/","title":"1. Overview Onebeat","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 15 de julho de 2025 13:16 Categoria: 1. Informativo \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:20 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: CS, DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 15 de julho de 2025 13:16 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Overview_Onebeat/#overview-do-onebeat","title":"Overview do Onebeat","text":""},{"location":"Overview_Onebeat/#introducao-ao-onebeat","title":"Introdu\u00e7\u00e3o ao Onebeat","text":"<p>O Onebeat \u00e9 uma ferramenta avan\u00e7ada de gest\u00e3o e an\u00e1lise de estoque e vendas, projetada para ajudar empresas a otimizar seus processos operacionais, tomar decis\u00f5es estrat\u00e9gicas e melhorar seus resultados financeiros.</p> <p>O sistema Onebeat busca encontrar a melhor forma de gerenciar a cadeia de estoque de uma empresa. Para isso, o sistema utiliza as informa\u00e7\u00f5es provenientes do cliente de Cadastros de Lojas e Produtos, e tamb\u00e9m informa\u00e7\u00f5es de quantidade de estoque e transa\u00e7\u00f5es de produtos.</p> <p>O sistema ent\u00e3o utiliza esses dados para fazer uma s\u00e9rie de c\u00e1lculos que ser\u00e3o utilizados para o gerenciamento do estoque dispon\u00edvel, gerando informa\u00e7\u00f5es como a quantidade de estoque por loja que deve ser abastecida, que est\u00e1 em excesso, que pode ser transferida, etc. Dando ao cliente uma vis\u00e3o geral de sua cadeia de suprimentos.</p>"},{"location":"Overview_Onebeat/#processo-de-integracao","title":"Processo de Integra\u00e7\u00e3o","text":"<p>A integra\u00e7\u00e3o \u00e9 o processo pelo qual conectamos os sistemas do cliente ao Onebeat. Este processo \u00e9 fundamental para garantir que os dados fluam corretamente e que a ferramenta possa realizar suas an\u00e1lises com precis\u00e3o.</p> <p>A integra\u00e7\u00e3o \u00e9 realizada atrav\u00e9s de c\u00f3digo Python e estruturas SQL (procedures, tabelas e views), onde fazemos a extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carregamento (ETL) dos dados do cliente para o formato compat\u00edvel com o Onebeat.</p> <p>Durante este processo, configuramos as tabelas padr\u00e3o do sistema e, quando necess\u00e1rio, criamos estruturas personalizadas para atender \u00e0s necessidades espec\u00edficas de cada cliente.</p> <p>Para entender como \u00e9 feito o processo de integra\u00e7\u00e3o do zero clique nesse link \u2014&gt; Integra\u00e7\u00e3o no Onebeat  </p>"},{"location":"Overview_Onebeat/#estrutura-do-banco-de-dados","title":"Estrutura do Banco de Dados","text":"<p>O banco de dados do Onebeat \u00e9 composto por tabelas e views padr\u00e3o, al\u00e9m de poss\u00edveis estruturas personalizadas para cada cliente. Abaixo est\u00e3o as principais tabelas padr\u00e3o do sistema:</p>"},{"location":"Overview_Onebeat/#tabelas-de-cadastro","title":"Tabelas de Cadastro","text":"<pre><code>-- Cadastros de SKUs (Stock Keeping Units)\nselect * from symphony_skus\n\n-- Cadastros de Localiza\u00e7\u00f5es (Lojas, Centros de Distribui\u00e7\u00e3o, etc.)\nselect * from symphony_stocklocations\n\n-- Rela\u00e7\u00f5es entre SKUs e Localiza\u00e7\u00f5es\nselect * from symphony_stocklocationskus\n\n-- Hist\u00f3rico das rela\u00e7\u00f5es entre SKUs e Localiza\u00e7\u00f5es\nselect * from symphony_stocklocationskuHistory\n\n</code></pre>"},{"location":"Overview_Onebeat/#tabelas-de-propriedades","title":"Tabelas de Propriedades","text":"<pre><code>-- Propriedades dos SKUs\nselect * from Symphony_SKUsProperty\n\n-- Valores das propriedades dos SKUs\nselect * from Symphony_SKUsPropertyItems\n\n-- Propriedades das Localiza\u00e7\u00f5es\nselect * from Symphony_StockLocationProperty\n\n-- Valores das propriedades das Localiza\u00e7\u00f5es\nselect * from Symphony_StockLocationPropertyItems\n\n</code></pre>"},{"location":"Overview_Onebeat/#tabelas-de-processamento-e-calculos","title":"Tabelas de Processamento e C\u00e1lculos","text":"<pre><code>-- Cadastro Location-Skus + Propriedades\nselect * from biss\n\n-- Cadastro Location-Skus C\u00e1lculos\nselect * from bissMTSSKUS\n\n-- Registros de integra\u00e7\u00e3o e logs\nselect * from Log_Integration\n\n</code></pre>"},{"location":"Overview_Onebeat/#fluxo-de-dados-no-onebeat","title":"Fluxo de Dados no Onebeat","text":"<ol> <li>Extra\u00e7\u00e3o de Dados: Coletamos informa\u00e7\u00f5es dos sistemas do cliente (sftp, api, view, etc.)</li> <li>Transforma\u00e7\u00e3o: Convertemos os dados para o formato padr\u00e3o do Onebeat atrav\u00e9s da integra\u00e7\u00e3o em python.</li> <li>Carregamento: Inserimos os dados nas tabelas apropriadas com o app do Onebeat (veja tudo a respeito do app \u2014&gt; App Onebeat informa\u00e7\u00f5es e configura\u00e7\u00f5es)</li> <li>Processamento: O Onebeat executa seus algoritmos de an\u00e1lise</li> <li>Visualiza\u00e7\u00e3o: Os resultados s\u00e3o disponibilizados atrav\u00e9s de dashboards e relat\u00f3rios (Power BI e/ou Analytics)</li> </ol>"},{"location":"Overview_Onebeat/#customizacoes-por-cliente","title":"Customiza\u00e7\u00f5es por Cliente","text":"<p>Al\u00e9m das estruturas padr\u00e3o, o Onebeat permite customiza\u00e7\u00f5es espec\u00edficas para cada cliente, como:</p> <ul> <li>Tabelas adicionais para dados espec\u00edficos do segmento</li> <li>Views personalizadas para relat\u00f3rios customizados</li> <li>Procedures espec\u00edficas para c\u00e1lculos ou regras de neg\u00f3cio particulares</li> <li>Integra\u00e7\u00f5es com sistemas propriet\u00e1rios</li> </ul>"},{"location":"Reprocessar_Carga/","title":"Como Reprocessar uma Carga","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 14 de julho de 2025 20:03 Categoria: Tutorial de Procedimento \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:19 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 14 de julho de 2025 20:03 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Reprocessar_Carga/#tutorial-como-reprocessar-uma-carga","title":"Tutorial: Como Reprocessar uma Carga","text":"<p>Este tutorial apresenta o passo a passo para reprocessar uma carga que n\u00e3o foi executada em uma data anterior. Siga as instru\u00e7\u00f5es abaixo para resolver este problema de forma eficiente.</p>"},{"location":"Reprocessar_Carga/#passo-1-verificar-o-motivo-da-falha-no-email-de-erros","title":"Passo 1: Verificar o motivo da falha no email de erros","text":"<p>O primeiro passo para reprocessar uma carga \u00e9 verificar no email de erros na data em quest\u00e3o o motivo pelo qual a carga n\u00e3o foi executada. Na maioria das vezes, a falha ocorre por dois motivos principais:</p> <ul> <li>Falha na gera\u00e7\u00e3o das interfaces (parcial ou total)</li> </ul> <p></p> <p>(Imagem exemplo, o erro pode variar)</p> <ul> <li>Falha no autoloader</li> </ul> <p></p> <p>Quer aprender a ler o email de erros? Clique no link para um tutorial detalhado! \u2014&gt;  Verificar os erros no email </p>"},{"location":"Reprocessar_Carga/#passo-2-resolver-falhas-nas-interfaces","title":"Passo 2: Resolver falhas nas interfaces","text":"<p>Se o problema for na gera\u00e7\u00e3o de interfaces, verifique se \u00e9 poss\u00edvel ger\u00e1-las novamente. O procedimento vai depender do cliente espec\u00edfico:</p>"},{"location":"Reprocessar_Carga/#para-interfaces-via-sftp","title":"Para interfaces via SFTP:","text":"<p>\u00c9 necess\u00e1rio contatar o cliente para que reenvie as interfaces caso n\u00e3o tenham sido enviadas ou enviadas incompletas.</p>"},{"location":"Reprocessar_Carga/#para-interfaces-via-view-ou-api","title":"Para interfaces via View ou API:","text":"<p>Na maioria dos casos, \u00e9 poss\u00edvel gerar as interfaces novamente atrav\u00e9s da altera\u00e7\u00e3o da vari\u00e1vel \"PROCESS_DATE\" e \"CURRENT_DATE\u201d, que a maioria dos clientes possui.</p> <p></p> <p>Basta alterar essa vari\u00e1vel para a data desejada para que as interfaces possam ser geradas. Em caso de reprocessamento de um cliente que utiliza sftp tem tamb\u00e9m a \"FTP_DATE\u201d:</p> <p></p> <p>Em caso de d\u00favidas sobre a gera\u00e7\u00e3o das interfaces, ou da aus\u00eancia da vari\u00e1vel, consulte o tech respons\u00e1vel pelo c\u00f3digo.</p> <p>Ap\u00f3s gerar as interfaces com sucesso, \u00e9 necess\u00e1rio executar o autoloader manualmente para processar os arquivos para a data desejada. Uma vez conclu\u00eddo o processamento pelo autoloader, voc\u00ea pode prosseguir para a atualiza\u00e7\u00e3o da BISS conforme descrito no Passo 4.</p>"},{"location":"Reprocessar_Carga/#passo-3-resolver-falhas-no-autoloader","title":"Passo 3: Resolver falhas no autoloader","text":"<p>Se o erro for apenas do autoloader e as interfaces foram geradas e carregadas no dia seguinte junto com a pr\u00f3xima carga, apenas a BISS precisa ser atualizada.</p> <p>Isso pode ser verificado no email de confirma\u00e7\u00e3o - geralmente a carga carrega 4 arquivos apenas. Caso tenha o dobro de arquivos usuais, significa que ocorreu esse tipo de situa\u00e7\u00e3o.</p> <p></p> <p>\u00c9 de extrema import\u00e2ncia verificar a pasta \"history folder\" para confirmar se os arquivos foram de fato carregados. TODOS os arquivos esperados devem estar presentes nesta pasta, E PREENCHIDOS COM DADOS, N\u00c3O PODEM ESYTAR VAZIOS, sem exce\u00e7\u00e3o. Esta verifica\u00e7\u00e3o \u00e9 um passo cr\u00edtico para garantir que o processamento foi completado corretamente e evitar problemas futuros relacionados a dados faltantes ou incompletos.</p> <p></p>"},{"location":"Reprocessar_Carga/#passo-4-atualizar-a-biss","title":"Passo 4: Atualizar a BISS","text":"<p>Observa\u00e7\u00e3o importante: Em ambos os casos (falha nas interfaces ou falha no autoloader), a BISS deve ser atualizada.</p>"},{"location":"Reprocessar_Carga/#verificando-a-existencia-da-procedure-sp_bisshistory","title":"Verificando a exist\u00eancia da procedure sp_BissHistory:","text":"<p>Verifique se o cliente tem a procedure sp_BissHistory, que \u00e9 respons\u00e1vel pela atualiza\u00e7\u00e3o da BISS para um dia que tenha sido deletada ou n\u00e3o tenha sido carregada.</p>"},{"location":"Reprocessar_Carga/#se-o-cliente-nao-tiver-a-sp_bisshistory","title":"Se o cliente n\u00e3o tiver a sp_BissHistory:","text":"<p>Ser\u00e1 necess\u00e1rio cri\u00e1-la. Acesse o tutorial espec\u00edfico para cria\u00e7\u00e3o da procedure neste link \u2014&gt; Criar sp_BissHistory </p>"},{"location":"Reprocessar_Carga/#se-o-cliente-tiver-a-sp_bisshistory","title":"Se o cliente tiver a sp_BissHistory:","text":"<p>Execute o comando abaixo para verificar o dia faltante:</p> <pre><code>use XXXDBOB -- complete com o banco certo\nDECLARE @dataInicial DATE = '2025-07-07'; -- data de consulta\nselect sum(inventoryAtSite),data from biss where data &gt;= @dataInicial group by data order by data desc\nselect sum(inventoryAtSite),updateDate from Symphony_StockLocationSkuHistory where updateDate &gt;= @dataInicial group by updateDate order by updateDate desc\n</code></pre> <p>Este comando compara a soma dos invent\u00e1rios da Symphony_StockLocationSkuHistory que \u00e9 atualizada ao fazer a carga no onebeat e a biss que deve ser atualizada pela procedure e possuir a mesma soma de inventario, venda etc.</p> <p>Ap\u00f3s identificar a data faltante, execute a procedure com o seguinte comando:</p> <pre><code>EXEC sp_BissHistory 'data_inicial', 'data_final'\n</code></pre> <p>Este comando executa a sp_BissHistory de uma data at\u00e9 outra. Se for apenas para um dia espec\u00edfico, utilize a mesma data como in\u00edcio e fim.</p> <p>Ap\u00f3s executar a procedure, \u00e9 essencial refazer o SELECT acima para confirmar se os dados foram inseridos corretamente na BISS. Verifique se a soma dos invent\u00e1rios na tabela BISS est\u00e1 agora alinhada com a tabela Symphony_StockLocationSkuHistory para a data que estava faltando. Esta verifica\u00e7\u00e3o garante que a atualiza\u00e7\u00e3o foi realizada com sucesso e que os dados est\u00e3o consistentes.</p>"},{"location":"Shared_Project/","title":"Shared Project","text":"<p>Criado por: Lucca Lacerda de Souza Lommez Criado em: 6 de agosto de 2025 11:25 Categoria: Documenta\u00e7\u00e3o \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 22 de agosto de 2025 18:40 Autor: Lucca Lacerda de Souza Lommez Direcionado ao: CS, DEV Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 6 de agosto de 2025 11:25 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Shared_Project/#documentacao-do-shared-project-overview","title":"Documenta\u00e7\u00e3o do Shared Project - Overview","text":""},{"location":"Shared_Project/#1-introducao-ao-shared-project","title":"1. Introdu\u00e7\u00e3o ao Shared Project","text":"<p>O Shared Project \u00e9 uma solu\u00e7\u00e3o de integra\u00e7\u00e3o e automa\u00e7\u00e3o desenvolvida para otimizar e padronizar processos de integra\u00e7\u00e3o entre sistemas. Diferente de abordagens tradicionais, onde cada cliente possui seu pr\u00f3prio c\u00f3digo de integra\u00e7\u00e3o isolado, o Shared Project centraliza fun\u00e7\u00f5es comuns em um \u00fanico reposit\u00f3rio compartilhado, permitindo que atualiza\u00e7\u00f5es e melhorias sejam aplicadas automaticamente a todos os clientes simultaneamente.</p>"},{"location":"Shared_Project/#conceito-fundamental","title":"Conceito Fundamental","text":"<p>Considere um cen\u00e1rio com 20 clientes diferentes, cada um com seu pr\u00f3prio processo de integra\u00e7\u00e3o. Anteriormente, caso fosse necess\u00e1rio implementar uma melhoria ou corrigir um erro, seria preciso modificar 20 c\u00f3digos diferentes. Com o Shared Project, \u00e9 necess\u00e1rio modificar apenas uma vez o c\u00f3digo central, e todos os 20 clientes s\u00e3o atualizados automaticamente na pr\u00f3xima execu\u00e7\u00e3o.</p>"},{"location":"Shared_Project/#por-que-ele-e-pratico","title":"Por que ele \u00e9 pr\u00e1tico?","text":"<ul> <li>Centraliza\u00e7\u00e3o de c\u00f3digo: Fun\u00e7\u00f5es comuns s\u00e3o mantidas em um \u00fanico lugar, eliminando duplica\u00e7\u00e3o</li> <li>Atualiza\u00e7\u00f5es autom\u00e1ticas: Cada cliente executa um \"pull\" do reposit\u00f3rio compartilhado diariamente</li> <li>Padroniza\u00e7\u00e3o: Todos os clientes seguem o mesmo padr\u00e3o de integra\u00e7\u00e3o</li> <li>Maior confiabilidade: Valida\u00e7\u00f5es e tratamentos de erros padronizados</li> <li>Redu\u00e7\u00e3o de tempo: Processos que antes demandavam configura\u00e7\u00e3o manual s\u00e3o automatizados</li> <li>Monitoramento centralizado: Todos os logs s\u00e3o enviados para um banco de dados central</li> </ul>"},{"location":"Shared_Project/#2-fluxo-de-trabalho","title":"2. Fluxo de Trabalho","text":""},{"location":"Shared_Project/#antes-do-shared-project","title":"Antes do Shared Project:","text":"<p>Anteriormente, cada cliente tinha seu pr\u00f3prio processo de integra\u00e7\u00e3o, com c\u00f3digos diferentes para tarefas comuns como:</p> <ul> <li>Gera\u00e7\u00e3o de interfaces (arquivos CSV)</li> <li>Carregamento de dados no sistema (Autoloader)</li> <li>Execu\u00e7\u00e3o de procedimentos no banco de dados</li> <li>Atualiza\u00e7\u00e3o de dashboards Power BI e/ou Analytics</li> <li>Gera\u00e7\u00e3o de relat\u00f3rios e logs</li> <li>Gerenciamento de arquivos</li> </ul> <p>Cada implementa\u00e7\u00e3o tinha suas particularidades, tratamentos de erros inconsistentes, e necessitava de manuten\u00e7\u00e3o individual. Qualquer melhoria precisava ser replicada manualmente em cada cliente.</p>"},{"location":"Shared_Project/#com-o-shared-project","title":"Com o Shared Project:","text":"<p>O processo agora \u00e9 padronizado e centralizado:</p> <ol> <li>O notebook principal do cliente importa o m\u00f3dulo Shared Project</li> <li>Executa-se automaticamente um \"git pull\" para garantir a vers\u00e3o mais atualizada</li> <li>As fun\u00e7\u00f5es padronizadas s\u00e3o utilizadas para todas as opera\u00e7\u00f5es comuns</li> <li>Valida\u00e7\u00f5es de ambiente ocorrem automaticamente</li> <li>Logs s\u00e3o padronizados e enviados para um banco central</li> <li>Processos longos s\u00e3o executados em threads separadas para otimiza\u00e7\u00e3o</li> </ol>"},{"location":"Shared_Project/#3-implementacao","title":"3. Implementa\u00e7\u00e3o","text":"<p>O processo de implementa\u00e7\u00e3o do Shared Project \u00e9 descrito detalhadamente em Integrar Shared Project  </p>"},{"location":"Shared_Project/#4-explicacao-das-funcoes","title":"4. Explica\u00e7\u00e3o das Fun\u00e7\u00f5es","text":""},{"location":"Shared_Project/#funcoes-de-inicializacao","title":"\u2014 Fun\u00e7\u00f5es de Inicializa\u00e7\u00e3o","text":""},{"location":"Shared_Project/#importlocalvars","title":"ImportLocalVars","text":"<p>Esta fun\u00e7\u00e3o importa vari\u00e1veis do notebook principal para o m\u00f3dulo compartilhado. Ela permite que o m\u00f3dulo compartilhado tenha acesso \u00e0s configura\u00e7\u00f5es espec\u00edficas de cada cliente sem precisar redefini-las.</p>"},{"location":"Shared_Project/#gitpull","title":"GitPull","text":"<p>Executa um \"git pull\" no reposit\u00f3rio Shared_Project para garantir que o c\u00f3digo mais recente seja utilizado. Esta fun\u00e7\u00e3o \u00e9 executada em uma thread separada com timeout para evitar travamentos, e registra o resultado no log de integra\u00e7\u00e3o.</p>"},{"location":"Shared_Project/#servervalidation","title":"ServerValidation","text":"<p>Verifica se o ambiente do servidor est\u00e1 configurado corretamente. Checa a exist\u00eancia da pasta de logs, as credenciais do Power BI e do banco central, e verifica se os servi\u00e7os Windows necess\u00e1rios est\u00e3o em execu\u00e7\u00e3o.</p>"},{"location":"Shared_Project/#funcoes-de-monitoramento-e-execucao","title":"\u2014 Fun\u00e7\u00f5es de Monitoramento e Execu\u00e7\u00e3o","text":""},{"location":"Shared_Project/#timecheck","title":"timeCheck","text":"<p>Uma classe que funciona como um decorador de contexto para medir o tempo de execu\u00e7\u00e3o de processos. Ela registra quando um processo come\u00e7a e termina, calculando o tempo total em minutos.</p>"},{"location":"Shared_Project/#sql_log","title":"sql_log","text":"<p>Registra eventos no banco de dados de log local e, opcionalmente, envia para o banco central. Esta fun\u00e7\u00e3o padroniza o formato dos logs.</p>"},{"location":"Shared_Project/#run_executable","title":"run_executable","text":"<p>Executa scripts externos (.ipynb, .py, .cmd) e captura sua sa\u00edda. Gera arquivos HTML para notebooks, facilitando a an\u00e1lise posterior. Mede o tempo de execu\u00e7\u00e3o e registra erros.</p>"},{"location":"Shared_Project/#exec_dbjobs","title":"exec_dbJobs","text":"<p>Executa uma lista de procedimentos armazenados no banco de dados, medindo o tempo de cada um. Trata erros espec\u00edficos e registra resultados.</p>"},{"location":"Shared_Project/#pbi_refresh","title":"pbi_refresh","text":"<p>Atualiza dashboards do Power BI usando a API oficial. Gerencia tokens de autentica\u00e7\u00e3o, verifica o status da atualiza\u00e7\u00e3o periodicamente e registra o resultado.</p>"},{"location":"Shared_Project/#funcoes-utilitarias","title":"\u2014 Fun\u00e7\u00f5es Utilit\u00e1rias","text":""},{"location":"Shared_Project/#clear_inputfolder","title":"clear_inputFolder","text":"<p>Move arquivos existentes da pasta de entrada para uma subpasta de arquivos ignorados, preparando o ambiente para uma nova execu\u00e7\u00e3o.</p>"},{"location":"Shared_Project/#to_sqlcsv","title":"to_SqlCsv","text":"<p>Exporta dados para SQL ou CSV com configura\u00e7\u00f5es padronizadas, garantindo consist\u00eancia em todas as exporta\u00e7\u00f5es.</p>"},{"location":"Shared_Project/#resetwindowsservices","title":"resetWindowsServices","text":"<p>Reinicia servi\u00e7os do Windows e processos, seguindo uma ordem espec\u00edfica para evitar problemas de depend\u00eancia.</p>"},{"location":"Shared_Project/#shutdownserver","title":"shutDownServer","text":"<p>Desliga o servidor ap\u00f3s verificar que todos os processos foram conclu\u00eddos com sucesso e que n\u00e3o h\u00e1 erros registrados.</p>"},{"location":"Shared_Project/#zipdeletefiles","title":"zipDeleteFiles","text":"<p>Compacta ou exclui arquivos antigos com base em sua data de modifica\u00e7\u00e3o, ajudando a gerenciar o espa\u00e7o em disco.</p>"},{"location":"Shared_Project/#funcoes-de-centralizacao","title":"\u2014 Fun\u00e7\u00f5es de Centraliza\u00e7\u00e3o","text":""},{"location":"Shared_Project/#getclientinfotocentral","title":"getClientInfoToCentral","text":"<p>Coleta informa\u00e7\u00f5es sobre a execu\u00e7\u00e3o atual (dados do Autoloader, quarentenas, arquivos importados, status de integra\u00e7\u00e3o) e envia para o banco de dados central, permitindo monitoramento unificado de todos os clientes.</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/","title":"Solu\u00e7\u00e3o de Proporcionalidade Pai-Filho (solution_multiples)","text":"<p>Criado por: Pedro Basilio Criado em: 29 de julho de 2025 04:57 Categoria: Tutorial de Procedimento \u00daltima edi\u00e7\u00e3o por: Lucca Lacerda de Souza Lommez \u00daltima atualiza\u00e7\u00e3o em: 8 de agosto de 2025 10:19 Autor: Pedro Basilio Direcionado ao: CS Lingua: Portugu\u00eas Status: Conclu\u00eddo Texto: 29 de julho de 2025 04:57 \u00daltima edi\u00e7\u00e3o por 1: Lucca Lacerda de Souza Lommez</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#objetivo-geral","title":"\ud83c\udfaf Objetivo Geral","text":"<p>Calcular o buffer ideal (estoque ideal) para SKUs filhos com base na propor\u00e7\u00e3o relativa a um SKU pai (ou grupo pai), usando diferentes estrat\u00e9gias de c\u00e1lculo. A solu\u00e7\u00e3o considera m\u00faltiplos clientes e todas as lojas do tipo \"POS\".</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#estrutura-de-tabelas","title":"\ud83d\udcc2 Estrutura de Tabelas","text":""},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#solution_multiples_input","title":"<code>solution_multiples_input</code>","text":"<p>Tabela de entrada manual/configur\u00e1vel pelo usu\u00e1rio.</p> Coluna Tipo Descri\u00e7\u00e3o id SERIAL PK clientid INTEGER ID do cliente modelcolor VARCHAR Chave do SKU (modelo/cor) father_code VARCHAR C\u00f3digo pai do SKU proportion_father_son_given NUMERIC Propor\u00e7\u00e3o manualmente atribu\u00edda proportion_logic INTEGER Estrat\u00e9gia de c\u00e1lculo:1: Margem2: Dado manual (Given)3: Buffer atual4: Rolling Sales 30d5: Rolling Sales 60d6: Hist\u00f3rico de Vendas7: Velocidade de Vendas min_dc_inv INTEGER Estoque m\u00ednimo no CD para considerar <p>Chave \u00fanica: <code>(clientid, modelcolor)</code></p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#solution_multiples_output","title":"<code>solution_multiples_output</code>","text":"<p>Tabela com os resultados finais do c\u00e1lculo do buffer.</p> Coluna Tipo Descri\u00e7\u00e3o id SERIAL PK clientid INTEGER Cliente locationid INTEGER Loja skuid INTEGER SKU modelcolor VARCHAR SKU (modelo/cor) father_code VARCHAR C\u00f3digo pai method_of_proportion_calc VARCHAR Nome da estrat\u00e9gia usada base_of_proportion_calc NUMERIC Valor base para a propor\u00e7\u00e3o proportion_father_son_calc NUMERIC Propor\u00e7\u00e3o filho/pai calculada buffer_son_calc INTEGER Buffer calculado para o filho buffer_father_calc INTEGER Buffer calculado para o pai"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#solution_multiples_input_view-e-solution_multiples_output_view","title":"<code>solution_multiples_input_view</code> e <code>solution_multiples_output_view</code>","text":"<p>Views para facilitar a leitura cruzada com as tabelas <code>vmc</code>, <code>sku</code>, <code>location</code> e <code>last_date_cleaned</code>.</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#procedure-sp_solution_multiplesid_clientid-sos_coverage-integer","title":"\u2699\ufe0f Procedure <code>sp_solution_multiples(id_clientid[], sos_coverage integer)</code>","text":""},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#funcao","title":"Fun\u00e7\u00e3o","text":"<p>Calcula buffers proporcionais por SKU filho, para todos os clientes da lista <code>id_clientid</code>, utilizando l\u00f3gica parametrizada e cobertura <code>sos_coverage</code>.</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#etapas-da-procedure","title":"Etapas da Procedure","text":""},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#1-limpeza-inicial","title":"1. Limpeza Inicial","text":"<pre><code>sql\nCopiarEditar\nDELETE FROM solution_multiples_output WHERE clientid = ANY(id_clientid);\n\n</code></pre>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#2-preparacao-das-bases-intermediarias-base1-a-base6","title":"2. Prepara\u00e7\u00e3o das bases intermedi\u00e1rias (base1 a base6)","text":"<ul> <li><code>base1</code> \u2192 Filtra os SKUs com <code>father_code</code></li> <li><code>base2</code> \u2192 Mapeia os SKUs v\u00e1lidos por <code>clientid</code></li> <li><code>base3</code> \u2192 Une base1 com skuid correspondente</li> <li><code>base4</code> \u2192 Faz produto cartesiano com todas as lojas \"POS\"</li> <li><code>base5</code> \u2192 Puxa dados de vendas e estoque da <code>last_date_cleaned</code></li> <li><code>base6</code> \u2192 Junta os dados, aplicando filtro por <code>stockatorigin &gt; min_dc_inv</code> (multiplicador = 1 ou 0)</li> </ul>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#3-calculo-do-total-do-pai-base7","title":"3. C\u00e1lculo do Total do Pai (base7)","text":"<p>Agrupa por <code>father_code</code> e <code>size_desc</code>, somando m\u00e9tricas ponderadas apenas para filhos v\u00e1lidos (<code>multiplier = 1</code>).</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#4-prepara-entrada-para-sp_sos-base8","title":"4. Prepara Entrada para <code>sp_sos</code> (base8)","text":"<p>Alimenta a procedure <code>sp_sos()</code> com dados de cobertura e estoque pai.</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#5-chamada-da-procedure-sp_sos","title":"5. Chamada da Procedure <code>sp_sos(...)</code>","text":"<p>Chama o m\u00e9todo respons\u00e1vel por distribuir o buffer ideal do pai entre os filhos (novo <code>optimalstock</code> por item pai).</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#6-une-resultado-com-novos-buffers-base9-a-base10","title":"6. Une Resultado com Novos Buffers (base9 a base10)","text":"<p>Enriquece com valores recalculados de estoque e velocidade de vendas.</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#7-calculo-da-proporcao-filhopai-base11","title":"7. C\u00e1lculo da Propor\u00e7\u00e3o Filho/Pai (base11)","text":"<p>Aplica a regra definida em <code>proportion_logic</code>:</p> <pre><code>sql\nCopiarEditar\nCASE\n  WHEN proportion_logic = 1 THEN margem / total_pai_margem\n  ...\n\n</code></pre>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#8-calculo-final-do-buffer-filho-base12","title":"8. C\u00e1lculo Final do Buffer Filho (base12)","text":"<pre><code>sql\nCopiarEditar\nbuffer_son_calc = GREATEST(FLOOR(new_optimalstock * son_proportion), 1)\n\n</code></pre>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#9-insercao-na-tabela-de-saida","title":"9. Inser\u00e7\u00e3o na Tabela de Sa\u00edda","text":"<p>Insere todos os dados na <code>solution_multiples_output</code>.</p>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#exemplo-de-uso","title":"\ud83d\udccc Exemplo de Uso","text":"<pre><code>sql\nCopiarEditar\n-- Inserir registros iniciais:\nINSERT INTO solution_multiples_input (...)\nSELECT ... FROM sku WHERE clientid = 17;\n\n-- Executar c\u00e1lculo para cliente 17 com cobertura de 30 dias:\nCALL sp_solution_multiples(array[17], 30);\n\n</code></pre>"},{"location":"Solucao_Proporcionalidade_Pai_Filho_solution_multiples/#observacoes-tecnicas","title":"\ud83e\udde0 Observa\u00e7\u00f5es T\u00e9cnicas","text":"<ul> <li>Garantia de 100%: O <code>multiplier</code> \u00e9 usado para excluir SKUs que n\u00e3o devem participar da propor\u00e7\u00e3o, garantindo que a soma das propor\u00e7\u00f5es seja 100%.</li> <li>Flexibilidade: Suporte a 7 m\u00e9todos diferentes de c\u00e1lculo de propor\u00e7\u00e3o.</li> <li>Modularidade: Estrutura baseada em tabelas tempor\u00e1rias e views que favorecem legibilidade e manuten\u00e7\u00e3o.</li> <li>Extensibilidade: Pode ser facilmente adaptado para novos m\u00e9todos de propor\u00e7\u00e3o ou filtros adicionais.</li> </ul>"}]}